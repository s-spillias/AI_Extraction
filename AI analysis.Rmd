---
title: "AI study analysis"
output:
  html_document: default
  pdf_document: default
date: "2024-04-04"
---

Initial Draft of Important Stats for CMS AI Project

Human data extraction data can be found at <https://docs.google.com/spreadsheets/d/1PI6LFSYbx4_N-S4slNqfrzDnTtcgCfAc9XOTRGlv3zU/edit#gid=1024178666>.

```{r setup, include=FALSE}
library(knitr)
library(dplyr)
library(tidyverse)
library(readxl)
library(beyonce)
library(ggpubr)
library(lme4)
library(lmerTest)
options(max.print = 999999)

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# open files - create dataframes #
file_path = 'output_assessment_CHATGPT_explanation.xlsx'
validate_path = list.files('Raw_Team_Assessment', pattern = 'validate', full.names = TRUE)
figure_folder = '../Manuscript/Figures'
# firstcontact-gpt4-32k
# January 29, ran Elicit and GPT4-32k
sheet_names <- excel_sheets(file_path)

all_data_list <- lapply(sheet_names, function(sheet) {
  read_excel(file_path, sheet = sheet) %>% 
    mutate(AI = sheet)})

do_stats = function(x){
  x %>% 
    summarise(mean = mean(value),
              standard_error = sd(value, na.rm = TRUE) / sqrt(n()),
              st_dev = sd(value),
              difficulty_mean = mean(Difficulty, na.rm = TRUE),
              st_dev_difficulty = sd(Difficulty, na.rm = TRUE),
              standard_error_difficulty = sd(Difficulty, na.rm = TRUE) / sqrt(n()),
    )}

difficulty <- read_excel("Extraction_Human_Difficulty.xlsx") %>% as_tibble() %>% 
  pivot_longer(-Question) %>% 
  setNames(c('Citation','Question','Difficulty')) %>% 
  mutate(Difficulty = case_match(Difficulty, 'Easy' ~ 1, 'Medium' ~ 2, 'Hard' ~ 3))

file_path = 'Extraction_ai.xlsx'
sheets = excel_sheets(file_path)
ai_extractions <- sheets %>% lapply(function(x) read_excel(file_path, sheet = x) %>% as_tibble() %>% 
                                      mutate(across(everything(), function(x) x %>% str_replace("Response: ",""))) %>%
                                      separate_wider_delim(-Citation, delim = "Context:", names = c('Response','Context'), names_sep = '***',too_few = "align_start") %>% 
                                      mutate(across(-Citation, nchar))) %>% setNames(sheets)

human_extraction = "Human_Extraction_Cleaned.xlsx" %>% 
  read_excel(sheet = 'Human') %>%
  select(-5, -(seq(8, ncol(.), by = 3))) %>%
  select(-1) %>% 
  slice(-(1:2)) %>% 
  mutate(across(-Question, nchar)) %>% 
  select(-last_col())

verbosity_ratios = sheets %>% lapply(function(x) bind_cols(human_extraction$Question, (ai_extractions[[x]] %>% select(-1))/(human_extraction %>% select(-1))))

reviewers = c(
  'R1', "Which country was the study conducted in?",
  'R1',"Provide some background as to the drivers and/or motivators of community-based fisheries management.",
  'R2',"What management mechanisms are used?",
  'R3',"Which groups of people are involved in the management as part of the CBFM case-studies? Choices: Community-members, Researchers, Practitioners, Government, NGOs, Other",
  'R4',"What benefits of Community-Based Fisheries Management are reported in this case study?",
  'R4',"What are the indicators of success of CBFM?",
  'R1',"How was the data on benefits collected?",
  'R3',"What are the reported barriers to success of Community-Based Fisheries Management?",
  'R2',"Guidelines for future implementation of CBFM?",
  'R3',"How does the community monitor the system they are managing?",
  'R4',"How does the community make decisions?")

extractor <- "Human_Extraction_Cleaned.xlsx" %>% 
  read_excel(sheet = 'Human') %>%
  dplyr::select(-5, -(seq(8, ncol(.), by = 3))) %>%
  #dplyr::select(-1) %>% 
  dplyr::slice(-(1:2)) %>% 
  mutate(across(-Question, nchar)) %>% 
  dplyr::select(-last_col()) %>% 
  select(-3:-24) %>% 
  mutate(short_cit = substr(Question, 1,35)) #no idea why citation column is actually the question column in this instance
colnames(extractor)[1] <- "Extractor"
colnames(extractor)[2] <- "Citation"
colnames(extractor)[3] <- "short_cit"

reviewers_id <- matrix(reviewers, nrow = length(reviewers) / 2, byrow = TRUE) %>% as_tibble() %>% 
  setNames(c("Reviewer","Question"))

df_description = all_data_list %>% bind_rows() %>% 
  pivot_longer(-c('Citation','AI')) %>% 
  separate(value, sep = ':::', into = c("value", "description")) %>% 
  separate(value, sep = "\\|", into = paste0("rep",1:5)) %>% 
  separate(description, sep = "\\|", into = paste0("Description",1:5)) %>% 
  pivot_longer(starts_with('rep'), names_to = 'Agent') 

ai_df <- df_description %>% 
  dplyr::select(-starts_with('Description')) %>% 
  separate(value, sep = ';', into = paste0('Crit', 1:4)) %>% 
  pivot_longer(starts_with('Crit'), names_to = 'Criteria')

team_path = "output_assessment_TEAM.xlsx" #original file with unedited flag values = output_assessment_TEAM_OG.xlsx

sheet_names <- excel_sheets(team_path)

team_data_list <- lapply(sheet_names, function(sheet) {
  read_excel(team_path, sheet = sheet) %>% 
    mutate(AI = sheet)
})

df_validate = validate_path %>% 
  lapply(function(x) sheet_names %>% lapply(function(y)    read_excel(x, sheet = y) %>% 
                                              mutate(AI = y, Reviewer = x %>% str_remove("Raw_Team_Assessment/validate_assessment_") %>% str_remove('.xlsx')) %>% 
                                              bind_rows())
  ) %>% bind_rows() %>% 
  pivot_longer(-c('Citation','AI','Reviewer'), names_to = 'Question',values_to = 'Value') %>% 
  filter(!(Value %>% str_detect("Response|Context"))) %>% 
  pivot_wider(names_from = 'Reviewer', values_from = 'Value') %>% 
  filter(!is.na(SCOTT)) %>% 
  pivot_longer(-c('Citation','AI','Question'), names_to = 'Reviewer',values_to = 'Value') %>% 
  separate(Value, sep = ';', into = paste0('Crit',1:4)) %>% 
  pivot_longer(starts_with('crit'),values_to = 'value', names_to = 'Criteria') %>% 
  mutate(value = as.numeric(value)) %>% 
  mutate(Reviewer = recode(Reviewer, 'MATT' = 'R1',
                           'FABIO' = 'R2',
                           'SCOTT' = 'R3',
                           'ROWAN' = 'R4'))

stat_summary <- team_data_list %>% bind_rows() %>% 
  pivot_longer(-c("Citation","AI")) %>% 
  separate(value, sep = ';', into = paste0('Crit',1:4)) %>% 
  pivot_longer(starts_with("Crit"), names_to = 'Criteria') %>% 
  mutate(Agent = 'Human') %>% 
  bind_rows(ai_df) %>% 
  mutate(Flag = ifelse(Criteria == 'Crit4' & value > 1, "FLAG",NA),
         group_agent = ifelse(str_detect(Agent, "Human"), "Human Assessor", "GPT4-Turbo Assessor"),
         value = as.integer(value)) %>% 
  {flag_df <<- .} %>% 
  filter(!is.na(value)) %>% 
  filter(Criteria != 'Crit4') %>%
  relocate(value, .after = 'group_agent') %>% 
  mutate(Criteria = case_match(Criteria, 'Crit1' ~ 'Context to Question',
                               'Crit2' ~ 'Response to Context',
                               'Crit3' ~ 'AI Response to Human Response')) %>% 
  rename('Question' = 'name') %>% 
  left_join(reviewers_id, by = 'Question') %>% 
  mutate(Reviewer = ifelse(str_detect(group_agent,'GPT'),'GPT4-Turbo',Reviewer)) %>% 
  mutate(short_cit = substr(Citation, 1,35)) %>% 
  left_join(difficulty %>%  mutate(short_cit = substr(Citation, 1,35)) %>% dplyr::select(-Citation), by = c("Question",'short_cit')) %>%
  mutate(AI = case_match(AI, 'ELICIT' ~ 'Elicit',
                         'OPENAI' ~ 'GPT4x3',
                         'OPENAI_single' ~ 'GPT4x1')) %>% 
  
  mutate(Question = factor(str_split_fixed(Question, '\\?',2)[,1],levels = str_split_fixed(reviewers_id$Question, '\\?',2)[,1])) %>% 
  {full_df <<- .} %>% 
  group_by(group_agent, AI,Criteria) %>% 
  do_stats()

full_df$value <- full_df$value - 2
stat_summary$mean <- stat_summary$mean - 2

full_df <- full_df %>%
  left_join(extractor, by = "short_cit") %>%
  mutate(Extractor = ifelse(group_agent == "Human Assessor", Extractor, "AI")) %>% 
  select(-Citation.y) %>% 
  rename(Citation = Citation.x)

question_df <- full_df %>% 
  group_by(Reviewer, AI, Criteria, Question) %>% 
  do_stats()

paper_df <- full_df %>% 
  group_by(group_agent, AI, Criteria, Citation) %>% 
  do_stats()

reviewer_df <- full_df %>% 
  group_by(Reviewer, Criteria) %>% 
  do_stats()

flag_df <- flag_df %>% filter(!is.na(Flag))



### Make Plots ####
crit_pal = beyonce_palette(127)[-1]
## Overall
ggplot(stat_summary, aes(x = AI, y = mean, fill = Criteria, group = Criteria)) +
  geom_hline(yintercept = 2, linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs( x = "AI Implementation", y = "Assessed Quality") +
  theme_classic() +
  coord_cartesian(ylim = c(1, 3)) +
  facet_wrap(~group_agent) +
  #scale_y_continuous(    labels = c("Poor","Fair","Good"),breaks = c(1,2,3),) +
  scale_fill_manual(values = crit_pal) +
  theme(legend.position = 'bottom') 
#ggsave(file.path(figure_folder,'overall.png'))


## Questions
question_df %>% filter(!str_detect(Reviewer,'GPT')) %>% ungroup() %>% 
  mutate(
    normalized_difficulty_mean = 1 + 2*((difficulty_mean - min(difficulty_mean,na.rm = TRUE)) / (max(difficulty_mean,na.rm = TRUE) - min(difficulty_mean,na.rm = TRUE))),
    normalized_difficulty_st_dev = st_dev_difficulty / (max(st_dev_difficulty) - min(st_dev_difficulty)),
    normalized_difficulty_standard_error = standard_error_difficulty / (max(standard_error_difficulty) - min(standard_error_difficulty))
  ) %>% 
  arrange(difficulty_mean) %>% ungroup() %>% 
  ggplot() +
  aes(x = AI, y = mean, fill = Criteria, group = Criteria) +
  geom_hline(yintercept = 2, linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_segment(aes(x = 4, y = 1.1, xend = 4, yend = 2.9), col = 'orange') +
  geom_point(aes(x = 4,y = normalized_difficulty_mean), size = 3, col = 'orange', show.legend = FALSE) +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  # labs(title = "Mean Values by Group and Criteria", x = "Group", y = "Mean Value") +
  theme_classic() +
  coord_cartesian(ylim = c(1, 3), xlim = c(1,3.7)) +
  facet_wrap(~Question, labeller = label_wrap_gen()) +
  labs(x = '') +
  scale_fill_manual(values = crit_pal) +
  scale_y_continuous(
    name = "Assessed Quality",
    #labels = c("Poor","Fair","Good"),breaks = c(1,2,3),
    sec.axis = sec_axis(~ (. - 1)/2, name = "Question Difficulty", labels = c("Easy","Hard"), breaks = c(0,1)),
  ) +
  theme(axis.text.y.right = element_text(color = "orange"),
        strip.text = element_text(size = 8),
        legend.position = 'bottom',
  ) +
  geom_text(aes(x = 4.2, y = 1.15, label = Reviewer, hjust = 1, vjust = 1), size = 3) 
#ggsave(file.path(figure_folder,'questions_supp.png'), width = 12, height = 7)

#Check for Reviewer Consistency and validation
ggplot(reviewer_df, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
  geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(x = "Reviewer", y = "Mean Assessed Quality") +
  theme_classic() +
  coord_cartesian(ylim = c(1, 3)) +
  theme(legend.position = 'bottom') +
  #facet_wrap(~Question) +
  scale_fill_manual(values = crit_pal)  # Set your desired fill colors
#ggsave(file.path(figure_folder,'reviewer_supp.png'), width = 12, height = 7)

df_validate_stats = df_validate %>%
  filter(Criteria !='Crit4') %>% 
  group_by(Criteria, Reviewer) %>% 
  summarise(mean = mean(value),
            standard_error = sd(value, na.rm = TRUE) / sqrt(n()),
            st_dev = sd(value))
ggplot(df_validate_stats, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
  geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(x = "Reviewer", y = "Mean Assessed Quality") +
  theme_classic() +
  coord_cartesian(ylim = c(1, 3)) +
  theme(legend.position = 'bottom') +
  #facet_wrap(~Question) +
  scale_fill_manual(values = crit_pal)

df_validate$value <- df_validate$value-2

df_validate %>% filter(Criteria != 'Crit4') %>% ggplot() +
  geom_bar(stat = 'identity', aes(x = Criteria, y = value, fill = Reviewer), position = 'dodge') +
  facet_wrap(AI~Question)

full_df %>% filter(!str_detect(Reviewer,'GPT')) %>% 
  group_by(Reviewer,Criteria) %>% 
  ggplot() +
  geom_density(fill = 'skyblue',aes(x = value), adjust = 2.5) +
  facet_grid(Reviewer~ Criteria) +
  theme_classic()

##Impact of Difficulty
ggplot(question_df %>% filter(!str_detect(Reviewer,'GPT')), aes(x = difficulty_mean, y = mean, col = Criteria)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +  # Use method = "lm" for linear model
  #stat_poly_line() +
  #stat_poly_eq() +
  theme_classic() +
  facet_wrap(~AI) +
  labs(y = 'Mean Assessed Quality', x = 'Mean Question Difficulty') +
  
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = crit_pal)
  #ggsave(file.path(figure_folder,'question_difficulty_supp.png'), width = 12, height = 7)


ggplot(paper_df %>% filter(str_detect(group_agent,'Human')), 
       aes(x = difficulty_mean, y = mean, col = AI)) +
  geom_point() +
  labs(y = 'Mean Assessed Quality', x = 'Mean Paper Difficulty') +
  
  geom_smooth(method = "lm", se = FALSE) +  # Use method = "lm" for linear model
  theme_classic() +
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = crit_pal) # Set your desired fill colors
  #ggsave(file.path(figure_folder,'paper_difficulty_supp.png'), width = 12, height = 7)

#### analysis ####
# combining median value for the 5 replicates of the AI assessor
humandf <- full_df %>% filter(Agent == "Human") %>% select(-short_cit)
aidf <- full_df %>% filter(Agent != "Human")
aidf2 <- aidf %>% group_by(Citation, AI, Question, Criteria, Flag, group_agent, Reviewer, Difficulty, Extractor) %>% 
  summarise(value = median(value)) %>% mutate(Agent = "AI") %>% 
  select(Citation, AI, Question, Criteria, Agent, Flag, group_agent, value, Reviewer, Difficulty, Extractor)
statdf <- rbind(humandf, aidf2)

# anova of this - may be INVALID due to distribution of data
aiaov <- aov(data = statdf, value ~ AI)
summary(aiaov)
tt <- TukeyHSD(aiaov)
tt
plot(aiaov)


# validating reviewers #
statdf2 <- statdf %>% filter(Reviewer != "GPT4-Turbo")
val <- aov( data = statdf2, value ~ Reviewer*Criteria)
summary(val)
valtt <- TukeyHSD(val)
valtt

df_validate_stats <- df_validate_stats %>% mutate(Criteria = case_match(Criteria, 'Crit1' ~ 'Context to Question',
                                                                        'Crit2' ~ 'Response to Context',
                                                                        'Crit3' ~ 'AI Response to Human Response'))
valplot <- {ggplot(df_validate_stats, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
    geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
    geom_col(stat = "identity", position = "dodge", color = "black") +
    geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                  position = position_dodge(width = 0.9), width = 0.25) +
    labs(x = "Reviewer", y = "Mean Assessed Quality") +
    theme_classic() +
    coord_cartesian(ylim = c(1, 3)) +
    theme(legend.position = 'bottom') +
    #facet_wrap(~Question) +
    scale_fill_manual(values = crit_pal)} #will need to run process_evaluations_new before to get objects

df_val <- df_validate %>% filter(Criteria != "Crit4") %>% mutate(Criteria = case_match(Criteria, 'Crit1' ~ 'Context to Question',
                                                                                       'Crit2' ~ 'Response to Context',
                                                                                       'Crit3' ~ 'AI Response to Human Response'))
crit1_mean <- df_val %>% filter(Criteria == "Context to Question") # crit 1 - in red
crit1_mean <- mean(crit1_mean$value)
print(crit1_mean) #mean = 2.458

crit2_mean <- df_val %>% filter(Criteria == "Response to Context") #crit 2 - in green
crit2_mean <- mean(crit2_mean$value)
print(crit2_mean) #mean = 2.533

crit3_mean <- df_val %>% filter(Criteria == "AI Response to Human Response") #crit 3 - in blue
crit3_mean <- mean(crit3_mean$value)
print(crit3_mean) #mean = 2.242

# attempting to create 5th "reviewer" which is all combined data - I think is valid (talk with Dave) but not best approach
df_val2 <- df_val %>% filter(Criteria != "NA") %>% mutate(Reviewer = "R5")
valbind <- rbind(df_val, df_val2)

valbind_stats = valbind %>% 
  group_by(Criteria, Reviewer) %>% 
  summarise(mean = mean(value),
            standard_error = sd(value, na.rm = TRUE) / sqrt(n()),
            st_dev = sd(value))
valplot2 <- {ggplot(valbind_stats, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
    geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
    geom_col(stat = "identity", position = "dodge", color = "black") +
    geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                  position = position_dodge(width = 0.9), width = 0.25) +
    labs(x = "Reviewer", y = "Mean Assessed Quality") +
    theme_classic() +
    coord_cartesian(ylim = c(1, 3)) +
    theme(legend.position = 'bottom') +
    #facet_wrap(~Question) +
    scale_fill_manual(values = crit_pal)} # compares individual reviewers (1-4) to the mean of all the values (R5)

valaov <- aov(data = valbind, value ~ Reviewer*Criteria)
summary(valaov)
val_hsd <- TukeyHSD(valaov)
val_hsd 
#only reviewer 1 is sig. diff. from "reviewer 5" (the average of all data) WHEN ALL CRITERIA COMBINED
#no interaction between reviewer*criteria (p = 0.222) - only sig. diff is R1 vs R3 response to context

## LMM analysis ##
library(lme4)
lmm_model <- lmer(value ~ AI + (1|Reviewer), data = statdf)
summary(lmm_model)
lmm_mod_anova <- anova(lmm_model)
#TukeyHSD(lmm_mod_anova)

## GLMM analysis ##
library(lme4)
library(emmeans)

statdfx <- statdf %>% filter(Agent == "Human")

# glmm_mod <- glmer(value ~ AI + (1|Reviewer), data = statdfx, family = poisson) #but is poisson legit? only one that works
# summary(glmm_mod)
# anova(glmm_mod)
# emm <- emmeans(glmm_mod, ~ AI)
# contrasts <- pairs(emm, adjust = "tukey")
# print(contrasts)
# 
# glmm_mod2 <- glmer(value ~ AI*Criteria + (1|Reviewer), data = statdf, family = poisson) #but is poisson legit? only one that works
# summary(glmm_mod2)
# anova(glmm_mod2)
# emm2 <- emmeans(glmm_mod2, ~ AI*Criteria)
# contrasts2 <- pairs(emm2, adjust = "tukey")
# print(contrasts2)
# 
# glmm_mod <- glmer(value ~ group_agent*AI + (1|Reviewer), data = statdf, family = poisson) #but is poisson legit? only one that works
# summary(glmm_mod)
# anova(glmm_mod)
# emm <- emmeans(glmm_mod, ~ group_agent*AI)
# contrasts <- pairs(emm, adjust = "tukey")
# print(contrasts)
# 
# mod_glmm <- glmer(value ~ AI*group_agent + (1|Reviewer), data = statdf, family = poisson)
# summary(mod_glmm)
# anova(mod_glmm)
# emm_glmm <- emmeans(mod_glmm, ~ AI|group_agent)
# pairs(emm_glmm)
# emm_int_glmm <- emmeans(mod_glmm, pairwise ~ AI:group_agent)
# pairs(emm_int_glmm)
# 
# # comparing full model with interaction to reduced model without - if p < 0.05, then interaction between AI and group_agent is sig.
# mod_red <- glmer(value ~ AI + group_agent + (1|Reviewer), data = statdf, family = poisson)
# anova_res <- anova(mod_red, mod_glmm)
# print(anova_res)
# 
# #### secondary analysis ####
# statdfx <- statdf %>% filter(Agent == "Human")
# xx <- glmer(value ~ AI + (1|Reviewer), family = poisson, data = filter(statdfx, Criteria == "AI Response to Human Response"))
# 
# #only comparing the value to the intercept with reviewer as the random effect 
# attempt1 <- lmer(value ~ 1 + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
# summary(attempt1)
# randoms <- ranef(attempt1, condVar = TRUE) #determining intercepts
# 
# #comparing the value by each AI (reviewer still random)
# attempt2 <- lmer(value ~ AI + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
# summary(attempt2)
# att2 <- emmeans(attempt2, ~ AI)
# contrasts <- pairs(att2, adjust = "tukey")
# print(contrasts)
# 
# #anova comparing the two models
# anova(attempt1, attempt2) #AIC lower for attempt 2, better, meaning AI has an effect
# 
# #now including question as a random effect
# attempt3 <- lmer(value ~ AI + (1|Question) + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
# anova(attempt2, attempt3)
# summary(attempt3)

#### PROPER MODEL TESTS ####
statdfx <- statdf %>% filter(Agent == "Human") #only investigating human assessors (no group agent)

#comparing distribution of values to the mean of 2
statdfx %>% group_by(AI, Criteria) %>% summarise(t_value = t.test(value, mu = 2, alternative = "less")$statistic)
statdfx %>%  group_by(AI, Criteria) %>%  summarise(t_value = t.test(value, mu = 2, alternative = "less")$statistic, p_value = t.test(value, mu = 2, alternative = "less")$p.value)
output1 <- statdfx %>%  group_by(AI, Criteria) %>%  summarise(t_value = t.test(value, mu = 2, alternative = "less")$statistic, p_value = t.test(value, mu = 2, alternative = "less")$p.value)
ungroup <- ungroup(output1)

#effect of difficulty on values for AI response to human response
diffdata <- paper_df %>% filter(group_agent != "GPT4-Turbo Assessor") %>% mutate(short_cit = substr(Citation, 1,35))
diffdata <- diffdata %>% left_join(select(extractor, short_cit, Extractor), by = c("short_cit")) 
diffdata <- subset(diffdata, select = -short_cit)
write.csv(diffdata, "diffdata.csv")
cor.test(diffdata$mean, diffdata$difficulty_mean) #correlation test to see if mean and difficulty mean are related (p = 0.48 - not sig.)

diffeffect <- lmer(mean ~ difficulty_mean + AI + (1|Citation), data = diffdata)
summ <- summary(diffeffect)
anova(diffeffect)

diff_null <- lmer(mean ~ difficulty_mean + (1|Citation), data = diffdata)
summary(diff_null)
anova(diff_null)

anova(diff_null, diffeffect)

diff_plot <- ggplot(diffdata, aes(x = difficulty_mean, y = mean, color = AI, shape = AI)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Difficulty Mean", y = "Mean", color = "AI Model", shape = "AI Model") +
  ggtitle("Relationship between Mean and Difficulty Mean by AI Model")

#effect of AI on values for AI response to human response
aieffect <- lmer(value ~ AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "Context to Question"))
summary(aieffect)
aieffect_emmeans <- emmeans(aieffect, ~ AI)
aieffect_cont <- pairs(aieffect_emmeans, adjust = "tukey")
print_aieffect <- print(aieffect_cont)

#effect of question on values for AI response to human response
qeffect <- lmer(value ~ Question + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
summary(qeffect)


```

## New stats

### T-test: Comparing values to the mean of 2
```{r comparing distribution, echo=FALSE}
ttest <- statdfx %>%  group_by(AI, Criteria) %>%  summarise(t_value = t.test(value, mu = 2, alternative = "two.sided")$statistic, p_value = t.test(value, mu = 2, alternative = "two.sided")$p.value)
print(ttest)

tplot <- ggplot(ttest, aes(x = Criteria, y = t_value, fill = AI)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  geom_text(aes(label = ifelse(p_value < 0.05, "*", "")), position = position_dodge(width = 0.9), vjust = -0.5) +
  labs(title = "Comparison to Mean of 2",
       x = "Criteria",
       y = "t-value") +
  theme_minimal()
plot(tplot)
```

### Effect of extractor on value and paper on value
```{r extractor and citation comparisons}
diffdata <- paper_df %>% filter(group_agent != "GPT4-Turbo Assessor") %>% mutate(short_cit = substr(Citation, 1,35))
diffdata <- diffdata %>% left_join(select(extractor, short_cit, Extractor), by = c("short_cit")) 
diffdata <- subset(diffdata, select = -short_cit)
write.csv(diffdata, "diffdata.csv")
cor.test(diffdata$mean, diffdata$difficulty_mean)

extrac_eff <- lmer(mean ~ Extractor + (1|Citation), data = diffdata)
anova(extrac_eff)

cit_eff <- lmer(mean ~ Citation + (1|Extractor), data = diffdata)
anova(cit_eff)
```

### Effect of AI on values (AI response to human response)
```{r AI to value, echo = TRUE}
fullai <- lmer(value ~ AI + (1|Reviewer) + (1|Citation), data = statdfx) #not looking at only one criteria
anova(fullai)
fullai_emmeans <- emmeans(fullai, ~ AI)
fullai_cont <- pairs(fullai_emmeans, adjust = "tukey")
print(fullai_cont)

fullai_crit <- lmer(value ~ AI*Criteria + (1|Reviewer) + (1|Citation), data = statdfx) #interaction w criteria
anova(fullai_crit)
fullai_crit_emmeans <- emmeans(fullai_crit, ~ AI*Criteria)
fullai_crit_cont <- pairs(fullai_crit_emmeans, adjust = "tukey")
print(fullai_crit_cont)

anova(fullai, fullai_crit)

# examining each criteria independently
aieffect <- lmer(value ~ AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
summary(aieffect)
anova(aieffect)
aieffect_emmeans <- emmeans(aieffect, ~ AI)
aieffect_cont <- pairs(aieffect_emmeans, adjust = "tukey")
print(aieffect_cont)

aieffect_rc <- lmer(value ~ AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "Response to Context"))
summary(aieffect_rc)
anova(aieffect_rc)
aieffect_rc_emmeans <- emmeans(aieffect_rc, ~ AI)
aieffect_rc_cont <- pairs(aieffect_rc_emmeans, adjust = "tukey")
print(aieffect_rc_cont)

aieffect_cq <- lmer(value ~ AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "Context to Question"))
summary(aieffect_cq)
anova(aieffect_cq)
aieffect_cq_emmeans <- emmeans(aieffect_cq, ~ AI)
aieffect_cq_cont <- pairs(aieffect_cq_emmeans, adjust = "tukey")
print(aieffect_cq_cont)
```

### Effect of the individual questions on assessment values
```{r effect of questions on value, echo = TRUE}
qeff.og <- lmer(value ~ Question + (1|Reviewer) + (1|Citation), data = statdf) #for all criteria
anova(qeff.og)
qeff.og_emmeans <- emmeans(qeff.og, ~ Question)
qeff.og_cont <- pairs(qeff.og_emmeans, adjust = "tukey")
print(qeff.og_cont)
kable(qeff.og_cont)
qeff_dataframe <- as.data.frame(qeff.og_cont)
write.csv(qeff_dataframe, "qeff_dataframe.csv")


qeffect <- lmer(value ~ Question + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
#summary(qeffect)
anova(qeffect)
qeff_emmeans <- emmeans(qeffect, ~Question)
qeff_cont <- pairs(qeff_emmeans, adjust = "tukey")
print(qeff_cont)
qeff_contdataframe <- as.data.frame(qeff_cont)
write.csv(qeff_contdataframe, "qeff_AIrespHumresp.csv")

qeff2 <- lmer(value ~ Question*Criteria + (1|Reviewer) + (1|Citation), data = statdf) #for all criteria
anova(qeff2)
qeff2_emmeans <- emmeans(qeff2, ~ Question*Criteria)
qeff2_cont <- pairs(qeff2_emmeans, adjust = "tukey")
print(qeff2_cont)

```

### Effect of difficulty on values

Also used a linear mixed model to determine if the assessment mean is affected by the relationship between the difficulty and the AI model, with the citation being a random effect
```{r diffeffect, echo = TRUE}
diff <- lmer(value ~ Difficulty*Criteria + (1|Citation) + (1|Extractor) + (1|Reviewer), data = statdfx)
anova(diff)
summary(diff)
# diff_emmeans <- emmeans(diff, ~AI)
# diff_cont <- pairs(diff_emmeans, adjust = "tukey")
# print(diff_cont)

diffdata <- question_df %>% filter(Reviewer != "GPT4-Turbo")

diffeffect <- lmer(mean ~ difficulty_mean*AI + (1|Question) + (1|Reviewer), data = diffdata) #better than null
# summ <- summary(diffeffect)
anova(diffeffect)
# diffeffect_emmeans <- emmeans(diffeffect, ~AI)
# diffeffect_cont <- pairs(diffeffect_emmeans, adjust = "tukey")
# print(diffeffect_cont)
# 
# diff_ai <- lmer(mean ~difficulty_mean + (1|Citation) + (1|Extractor), data = filter(diffdata, AI == "GPT4x3"))
# anova(diff_ai)
# 
# diff_null <- lmer(mean ~ difficulty_mean + (1|Citation) + (1|Extractor), data = diffdata)
# summ2 <- summary(diff_null)
# anova(diff_null)
# 
# diffanova <- anova(diff_null, diffeffect)
# print(diffanova)
```


### Effect of assessor on assesed quality (GPT4 vs human)
```{r effect if assessor on value}
assessor <- lmer(value ~ group_agent + (1|Extractor) + (1|Reviewer), data = full_df)
anova(assessor)
```


### Key Plots
#### overall plot showing all data
```{r overall}
overall <- ggplot(stat_summary, aes(x = AI, y = mean, fill = Criteria, group = Criteria)) +
  geom_hline(yintercept = 2, linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs( x = "AI Implementation", y = "Assessed Quality") +
  theme_classic() +
  coord_cartesian(ylim = c(1, 3)) +
  facet_wrap(~group_agent) +
  #scale_y_continuous(    labels = c("Poor","Fair","Good"),breaks = c(1,2,3),) +
  scale_fill_manual(values = crit_pal) +
  theme(legend.position = 'bottom') 
plot(overall)

ggsave("Figures/overall.png", width = 7, height = 5, device = "png")
```

#### plot showing only human assessor data by AI
```{r human only}
stat_summary_hum <- stat_summary %>% filter(group_agent == "Human Assessor")
stat_summary_hum$significance <- ifelse(stat_summary_hum$AI == "GPT4x1" & stat_summary_hum$Criteria %in% c("AI Response to Human Response", "Context to Question"), "*", "")
(human <- ggplot(stat_summary_hum, aes(x = AI, y = mean, fill = Criteria, group = Criteria)) +
  geom_hline(yintercept = 0, linetype = 'dashed', alpha = 0.5) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  geom_text(aes(label = significance, y = 1.25), position = position_dodge(width = 0.9), vjust = 0, size = 10) +
  geom_text(aes(label = ifelse(AI == "Elicit" & Criteria == "AI Response to Human Response", "a", "")),
            x = 1.3, y = 0.6, vjust = 0.5, size = 5, color = "black") +
  geom_text(aes(label = ifelse(AI == "GPT4x1" & Criteria == "Response to Context", "b", "")),
            x = 2.3, y = 0.3, vjust = 0.5, size = 5, color = "black") +
  geom_text(aes(label = ifelse(AI == "GPT4x3" & Criteria == "Response to Context", "b", "")),
          x = 3.3, y = 0.42, vjust = 0.5, size = 5, color = "black") +
  labs(x = "AI Implementation", y = "Assessed Quality") +
  theme_classic() +
  coord_cartesian(ylim = c(-1, 1)) +
  #scale_fill_manual(values = crit_pal) +
  theme(legend.position = 'bottom'))

ggsave("Figures/human.png", width = 7, height = 5, device = "png")
```

#### plot showing breakdown of assessment values by questions with overall difficulty
```{r questions}
q_plot <- question_df %>% filter(!str_detect(Reviewer,'GPT')) %>% ungroup() %>% 
  mutate(
    normalized_difficulty_mean = 1 + 2*((difficulty_mean - min(difficulty_mean,na.rm = TRUE)) / (max(difficulty_mean,na.rm = TRUE) - min(difficulty_mean,na.rm = TRUE))),
    normalized_difficulty_st_dev = st_dev_difficulty / (max(st_dev_difficulty) - min(st_dev_difficulty)),
    normalized_difficulty_standard_error = standard_error_difficulty / (max(standard_error_difficulty) - min(standard_error_difficulty))
  ) %>% 
  arrange(difficulty_mean) %>% ungroup() %>% 
  ggplot() +
  aes(x = AI, y = mean, fill = Criteria, group = Criteria) +
  geom_hline(yintercept = 0, linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_segment(aes(x = 4, y = 1.1, xend = 4, yend = 2.9), col = 'orange') +
  geom_point(aes(x = 4,y = normalized_difficulty_mean), size = 3, col = 'orange', show.legend = FALSE) +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  # labs(title = "Mean Values by Group and Criteria", x = "Group", y = "Mean Value") +
  theme_classic() +
  coord_cartesian(ylim = c(-1, 1), xlim = c(1,3.7)) +
  facet_wrap(~Question, labeller = label_wrap_gen()) +
  labs(x = '') +
  scale_fill_manual(values = crit_pal) +
  scale_y_continuous(
    name = "Assessed Quality",
    #labels = c("Poor","Fair","Good"),breaks = c(1,2,3),
    sec.axis = sec_axis(~ (. - 1)/2, name = "Question Difficulty", labels = c("Easy","Hard"), breaks = c(0,1)),
  ) +
  theme(axis.text.y.right = element_text(color = "orange"),
        strip.text = element_text(size = 8),
        legend.position = 'bottom',
  ) +
  geom_text(aes(x = 4.2, y = 1.15, label = Reviewer, hjust = 1, vjust = 1), size = 3)
plot(q_plot)

ggsave("Figures/questions.png", width = 9, height = 6, device = "png")

```

#### plot showing overall difficulty
```{r difficulty plot, echo = FALSE}
diff_plot <- ggplot(diffdata, aes(x = difficulty_mean, y = mean, color = AI, shape = AI)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Difficulty Mean", y = "Mean", color = "AI Model", shape = "AI Model") +
  ggtitle("Relationship between Mean and Difficulty Mean by AI Model")
print(diff_plot)

diff_plot2 <- ggplot(statdfx, aes(x = Difficulty, y = value, color = AI, shape = AI)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Difficulty", y = "Value", color = "AI Model", shape = "AI Model") +
  ggtitle("Relationship between Mean and Difficulty Mean by AI Model")
#print(diff_plot2) this plot is irrelevant because it's only 9 points


```

#### plot showing difficulty broken up by each AI model
```{r difficulty by AI}
diffbyai <- ggplot(question_df %>% filter(!str_detect(Reviewer,'GPT')), aes(x = difficulty_mean, y = mean, col = Criteria)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +  # Use method = "lm" for linear model
  #stat_poly_line() +
  #stat_poly_eq() +
  theme_classic() +
  facet_wrap(~AI) +
  labs(y = 'Mean Assessed Quality', x = 'Mean Question Difficulty') +
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = crit_pal)
plot(diffbyai)

ggsave("Figures/diff_by_ai.png", width = 7, height = 5, device = "png")
```

#### plot showing how reviewers ranked the same dataset, compared to AI (gpt4-turbo)
```{r reviewer consistency}
rev <- ggplot(reviewer_df, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
  geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(x = "Reviewer", y = "Mean Assessed Quality") +
  theme_classic() +
  coord_cartesian(ylim = c(-1, 1)) +
  theme(legend.position = 'bottom') +
  #facet_wrap(~Question) +
  scale_fill_manual(values = crit_pal)  # Set your desired fill colors
plot(rev)
```

#### plot showing how reviewers ranked the their assigned dataset (I believe)
```{r reviewers real data}
rev2 <- ggplot(df_validate_stats, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
  geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(x = "Reviewer", y = "Mean Assessed Quality") +
  theme_classic() +
  coord_cartesian(ylim = c(-1, 1)) +
  theme(legend.position = 'bottom') +
  #facet_wrap(~Question) +
  scale_fill_manual(values = crit_pal) 
plot(rev2)
```

### Confusion Matrix
```{r confusion matrix}
file_path = 'Extraction_ai.xlsx'
sheets = excel_sheets(file_path)
ai_extractions <- sheets %>% lapply(function(x) read_excel(file_path, sheet = x) %>% as_tibble() %>% 
    mutate(across(everything(), function(x) x %>% str_replace("Response: ",""))) %>%
    separate_wider_delim(-Citation, delim = "Context: ", names = c('Response','Context'), names_sep = '***',too_few = "align_start")) %>% 
  setNames(sheets)

GPT4x1<- ai_extractions[['OPENAI_single']] %>% mutate(Citation = substr(Citation, 1,35))
GPT4x3 <- ai_extractions[['OPENAI']] %>% mutate(Citation = substr(Citation, 1,35))
Elicit <- ai_extractions[['ELICIT']] %>% mutate(Citation = substr(Citation, 1,35))

human_extraction = "Human_Extraction_Cleaned.xlsx" %>% 
  read_excel(sheet = 'Human') %>%
  select(-5, -(seq(8, ncol(.), by = 3))) %>%
  slice(-(1:2))
human_extraction <- human_extraction[,c(2:24, 1, 25)]
names(human_extraction) <- names(GPT4x1)
colnames(human_extraction)[24] <- "Human"
colnames(human_extraction)[25] <- "Notes"

human <- subset(human_extraction, select = -c(Human, Notes))
human <- human %>% mutate(Citation = substr(Citation, 1,35))

human_long <- human  %>% 
  pivot_longer(cols = -Citation, 
               names_to = "Question",
               values_to = "Answer") %>% 
  mutate(Source="human") %>%
  select(Citation, Question, Source, Answer)

GPT4x1_long <- GPT4x1 %>% 
  pivot_longer(cols = -Citation, 
               names_to = "Question",
               values_to = "Answer") %>% 
  mutate(Source="GPT4x1") %>%
  select(Citation, Question, Source, Answer)

GPT4x3_long <- GPT4x3  %>% 
  pivot_longer(cols = -Citation, 
               names_to = "Question",
               values_to = "Answer") %>% 
  mutate(Source="GPT4x3") %>%  
  select(Citation, Question, Source, Answer)

Elicit_long <- Elicit  %>% 
  pivot_longer(cols = -Citation, 
               names_to = "Question",
               values_to = "Answer") %>% 
  mutate(Source="Elicit") %>%  
  select(Citation, Question, Source, Answer)

fulldf <- bind_rows(human_long, GPT4x1_long, GPT4x3_long, Elicit_long)
widedf <- fulldf %>% pivot_wider(names_from = 'Source', values_from = 'Answer')

widedf[] <- lapply(widedf, trimws)
df <- widedf %>% 
  mutate_all(~ifelse(. == "NA", NA, .)) %>% 
  mutate_all(~ifelse(. == "NA***", NA, .)) %>% 
  mutate_all(~ifelse(. == "NA; NA; NA", NA, .)) %>% 
  mutate_all(~ifelse(. == "NO DATA", NA, .)) %>%
  mutate_all(~ifelse(. == "NO CONTEXT;  NO CONTEXT;  NO CONTEXT", NA, .)) %>% 
  mutate_all(~ifelse(. == "NO CONTEXT;  NO CONTEXT; NO DATA", NA, .)) %>%
  mutate_all(~ifelse(. == "NO DATA;  NO CONTEXT;  NO CONTEXT", NA, .))
newdf <- df %>% mutate(Category = ifelse(grepl("Response", Question,), "response", "context"))

pal = c("#a6611a",
        "#dfc27d",
        "#80cdc1",
        "#018571")

(newdfx <- newdf %>% pivot_longer(c(GPT4x1, GPT4x3, Elicit), names_to = "AI", values_to = "value") %>% 
    mutate(human_data = ifelse(is.na(human),"No Data","Data"),
           ai_data = ifelse(is.na(value),"No Data","Data")) %>% 
    mutate(human_data = factor(human_data,levels = c('Data',"No Data")),
           ai_data = factor(ai_data, levels = c("Data","No Data") %>% rev)) %>% 
    group_by(AI,human_data,ai_data,Category) %>% 
    summarise(value =  signif(n()/363, digits = 2)) %>% as_tibble() %>% 
    complete(AI,human_data,ai_data,Category, fill = list(value = 0)) %>% 
    ungroup() %>% 
    mutate(display_value = ifelse(value >= 0.01 | value == 0, value,"<0.01")) %>% 
  ggplot() +
  geom_tile(aes(x = human_data, y = ai_data, fill = interaction(human_data,ai_data))) +
  geom_text(aes(x = human_data, y = ai_data, label = display_value)) +
  xlab("Human") +
  ylab("AI") +
   scale_fill_manual(values = pal[c(1,4,4,2)]) +
  facet_wrap(Category ~ AI) +
  theme_classic() +
  theme(legend.position = "none"))

# ggsave("Figures/confusion_matrix.png", width = 7, height = 5, device = "png")
```







## New stats - AI Response to Human Response

### T-test: Comparing values to the mean of 2
```{r comparing distribution, echo=FALSE}
ttest <- statdfx %>% filter (Criteria == "AI Response to Human Response") %>% group_by(AI) %>%  summarise(t_value = t.test(value, mu = 0, alternative = "two.sided")$statistic, p_value = t.test(value, mu = 0, alternative = "two.sided")$p.value)
print(ttest)

tplot <- ggplot(ttest, aes(x = AI, y = t_value)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  geom_text(aes(label = ifelse(p_value < 0.05, "*", "")), position = position_dodge(width = 0.9), vjust = -0.5) +
  labs(title = "Comparison to Mean of 2",
       x = "AI",
       y = "t-value") +
  theme_minimal()
plot(tplot)
```

### Effect of extractor on value and paper on value
```{r extractor and citation comparisons}
diffdata <- paper_df %>% filter(group_agent != "GPT4-Turbo Assessor") %>% mutate(short_cit = substr(Citation, 1,35))
diffdata <- diffdata %>% left_join(select(extractor, short_cit, Extractor), by = c("short_cit")) 
diffdata <- subset(diffdata, select = -short_cit)
write.csv(diffdata, "diffdata.csv")
cor.test(diffdata$mean, diffdata$difficulty_mean)

#Extractor
extrac_eff <- lmer(mean ~ Extractor + (1|Citation), data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(extrac_eff)

model_extractor <- lm(mean ~ Extractor, data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(model_extractor)

#Citation
cit_eff <- lmer(mean ~ Citation + (1|Extractor), data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(cit_eff)

model_citation <- lm(mean ~ Citation, data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(model_citation)


```

### Effect of AI on values (AI response to human response)
```{r AI to value, echo = TRUE}
# examining each criteria independently
aieffect <- lmer(value ~ AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
summary(aieffect)
anova(aieffect)
aieffect_emmeans <- emmeans(aieffect, ~ AI)
aieffect_cont <- pairs(aieffect_emmeans, adjust = "tukey")
print(aieffect_cont)
```

### Effect of the individual questions on assessment values
```{r effect of questions on value, echo = TRUE}
qeffect <- lmer(value ~ Question + (1|Reviewer) + (1|Citation) + (1|AI), data = filter(statdfx, Criteria == "AI Response to Human Response"))
#summary(qeffect)
anova(qeffect)
qeff_emmeans <- emmeans(qeffect, ~Question)
qeff_cont <- pairs(qeff_emmeans, adjust = "tukey")
print(qeff_cont)
qeff_contdataframe <- as.data.frame(qeff_cont)
write.csv(qeff_contdataframe, "qeff_AIrespHumresp.csv")

#this model includes AI
qeffect <- lmer(value ~ Question*AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
#summary(qeffect)
anova(qeffect)
qeff_emmeans <- emmeans(qeffect, ~Question*AI)
qeff_cont <- pairs(qeff_emmeans, adjust = "tukey")
print(qeff_cont)
qeff_contdataframe <- as.data.frame(qeff_cont)
write.csv(qeff_contdataframe, "qeff_AIrespHumresp.csv")
```

### Effect of difficulty on values

Also used a linear mixed model to determine if the assessment mean is affected by the relationship between the difficulty and the AI model, with the citation being a random effect
```{r diffeffect, echo = TRUE}
diff <- lmer(value ~ Difficulty*AI + (1|Citation) + (1|Extractor) + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
anova(diff)
summary(diff)
# diff_emmeans <- emmeans(diff, ~AI)
# diff_cont <- pairs(diff_emmeans, adjust = "tukey")
# print(diff_cont)

# ALL CRITERIA - JUST FOR COMPARISON
diff <- lmer(value ~ Difficulty*Criteria + (1|Citation) + (1|Extractor) + (1|Reviewer), data = statdfx) 
anova(diff)

```


### Effect of assessor on assesed quality (GPT4 vs human)
```{r effect if assessor on value}
assessor <- lmer(value ~ group_agent + (1|Extractor) + (1|Reviewer), data = full_df)
anova(assessor)
```


### Key Plots
#### overall plot showing all data
```{r overall}
overall <- ggplot(stat_summary, aes(x = AI, y = mean, fill = Criteria, group = Criteria)) +
  geom_hline(yintercept = 2, linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs( x = "AI Implementation", y = "Assessed Quality") +
  theme_classic() +
  coord_cartesian(ylim = c(-0.5, 1)) +
  facet_wrap(~group_agent) +
  #scale_y_continuous(    labels = c("Poor","Fair","Good"),breaks = c(1,2,3),) +
  scale_fill_manual(values = crit_pal) +
  theme(legend.position = 'bottom') 
plot(overall)

ggsave("Figures/overall.png", width = 7, height = 5, device = "png")
```

#### plot showing only human assessor data by AI
```{r human only}
stat_summary_hum <- stat_summary %>% filter(group_agent == "Human Assessor") %>% filter(Criteria == "AI Response to Human Response")
stat_summary_hum$significance <- ifelse(stat_summary_hum$AI == "GPT4x1" & stat_summary_hum$Criteria %in% c("AI Response to Human Response", "Context to Question"), "*", "")
(human <- ggplot(stat_summary_hum, aes(x = AI, y = mean, fill = Criteria, group = Criteria)) +
    geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = 0), fill = "#FFC0CB", alpha = 0.1) +
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = 0, ymax = Inf), fill = "#98FB98", alpha = 0.1) +
  geom_hline(yintercept = 0, linetype = 'dashed', alpha = 0.5) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  geom_text(aes(label = significance, y = 1.25), position = position_dodge(width = 0.9), vjust = 0, size = 10) +
  # geom_text(aes(label = ifelse(AI == "Elicit" & Criteria == "AI Response to Human Response", "a", "")),
  #          x = 1.3, y = 0.6, vjust = 0.5, size = 5, color = "black") +
  # geom_text(aes(label = ifelse(AI == "GPT4x1" & Criteria == "Response to Context", "b", "")),
  #           x = 2.3, y = 0.3, vjust = 0.5, size = 5, color = "black") +
  # geom_text(aes(label = ifelse(AI == "GPT4x3" & Criteria == "Response to Context", "b", "")),
  #         x = 3.3, y = 0.42, vjust = 0.5, size = 5, color = "black") +
  labs(x = "AI Implementation", y = "Assessed Quality") +
  theme_classic() +
  coord_cartesian(ylim = c(-0.25, 0.5)) +
  scale_fill_manual(values = crit_pal) +
  theme(legend.position = 'bottom'))

ggsave("Figures/human.png", width = 7, height = 5, device = "png")
```

#### plot showing breakdown of assessment values by questions with overall difficulty
```{r questions}
q_plot <- question_df %>% filter(!str_detect(Reviewer,'GPT')) %>% ungroup() %>% 
  mutate(
    normalized_difficulty_mean = 1 + 2*((difficulty_mean - min(difficulty_mean,na.rm = TRUE)) / (max(difficulty_mean,na.rm = TRUE) - min(difficulty_mean,na.rm = TRUE))),
    normalized_difficulty_st_dev = st_dev_difficulty / (max(st_dev_difficulty) - min(st_dev_difficulty)),
    normalized_difficulty_standard_error = standard_error_difficulty / (max(standard_error_difficulty) - min(standard_error_difficulty))
  ) %>% 
  arrange(difficulty_mean) %>% ungroup() %>% 
  ggplot() +
  aes(x = AI, y = mean, fill = Criteria, group = Criteria) +
  geom_hline(yintercept = 2, linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_segment(aes(x = 4, y = 1.1, xend = 4, yend = 2.9), col = 'orange') +
  geom_point(aes(x = 4,y = normalized_difficulty_mean), size = 3, col = 'orange', show.legend = FALSE) +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  # labs(title = "Mean Values by Group and Criteria", x = "Group", y = "Mean Value") +
  theme_classic() +
  coord_cartesian(ylim = c(1, 3), xlim = c(1,3.7)) +
  facet_wrap(~Question, labeller = label_wrap_gen()) +
  labs(x = '') +
  scale_fill_manual(values = crit_pal) +
  scale_y_continuous(
    name = "Assessed Quality",
    #labels = c("Poor","Fair","Good"),breaks = c(1,2,3),
    sec.axis = sec_axis(~ (. - 1)/2, name = "Question Difficulty", labels = c("Easy","Hard"), breaks = c(0,1)),
  ) +
  theme(axis.text.y.right = element_text(color = "orange"),
        strip.text = element_text(size = 8),
        legend.position = 'bottom',
  ) +
  geom_text(aes(x = 4.2, y = 1.15, label = Reviewer, hjust = 1, vjust = 1), size = 3)
plot(q_plot)

ggsave("Figures/questions.png", width = 9, height = 6, device = "png")

```

#### plot showing overall difficulty
```{r difficulty plot, echo = FALSE}
diff_plot <- ggplot(diffdata, aes(x = difficulty_mean, y = mean, color = AI, shape = AI)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Difficulty Mean", y = "Mean", color = "AI Model", shape = "AI Model") +
  ggtitle("Relationship between Mean and Difficulty Mean by AI Model")
print(diff_plot)

diff_plot2 <- ggplot(statdfx, aes(x = Difficulty, y = value, color = AI, shape = AI)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Difficulty", y = "Value", color = "AI Model", shape = "AI Model") +
  ggtitle("Relationship between Mean and Difficulty Mean by AI Model")
#print(diff_plot2) this plot is irrelevant because it's only 9 points


```

#### plot showing difficulty broken up by each AI model
```{r difficulty by AI}
diffbyai <- ggplot(question_df %>% filter(!str_detect(Reviewer,'GPT')), aes(x = difficulty_mean, y = mean, col = Criteria)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +  # Use method = "lm" for linear model
  #stat_poly_line() +
  #stat_poly_eq() +
  theme_classic() +
  facet_wrap(~AI) +
  labs(y = 'Mean Assessed Quality', x = 'Mean Question Difficulty') +
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = crit_pal)
plot(diffbyai)

ggsave("Figures/diff_by_ai.png", width = 7, height = 5, device = "png")
```

#### plot showing how reviewers ranked the same dataset, compared to AI (gpt4-turbo)
```{r reviewer consistency}
rev <- ggplot(reviewer_df, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
  geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(x = "Reviewer", y = "Mean Assessed Quality") +
  theme_classic() +
  coord_cartesian(ylim = c(1, 3)) +
  theme(legend.position = 'bottom') +
  #facet_wrap(~Question) +
  scale_fill_manual(values = crit_pal)  # Set your desired fill colors
plot(rev)
```

#### plot showing how reviewers ranked the their assigned dataset (I believe)
```{r reviewers real data}
rev2 <- ggplot(df_validate_stats, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
  geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(x = "Reviewer", y = "Mean Assessed Quality") +
  theme_classic() +
  coord_cartesian(ylim = c(1, 3)) +
  theme(legend.position = 'bottom') +
  #facet_wrap(~Question) +
  scale_fill_manual(values = crit_pal) 
plot(rev2)
```

### Confusion Matrix
```{r confusion matrix}
file_path = 'Extraction_ai.xlsx'
sheets = excel_sheets(file_path)
ai_extractions <- sheets %>% lapply(function(x) read_excel(file_path, sheet = x) %>% as_tibble() %>% 
    mutate(across(everything(), function(x) x %>% str_replace("Response: ",""))) %>%
    separate_wider_delim(-Citation, delim = "Context: ", names = c('Response','Context'), names_sep = '***',too_few = "align_start")) %>% 
  setNames(sheets)

GPT4x1<- ai_extractions[['OPENAI_single']] %>% mutate(Citation = substr(Citation, 1,35))
GPT4x3 <- ai_extractions[['OPENAI']] %>% mutate(Citation = substr(Citation, 1,35))
Elicit <- ai_extractions[['ELICIT']] %>% mutate(Citation = substr(Citation, 1,35))

human_extraction = "Human_Extraction_Cleaned.xlsx" %>% 
  read_excel(sheet = 'Human') %>%
  select(-5, -(seq(8, ncol(.), by = 3))) %>%
  slice(-(1:2))
human_extraction <- human_extraction[,c(2:24, 1, 25)]
names(human_extraction) <- names(GPT4x1)
colnames(human_extraction)[24] <- "Human"
colnames(human_extraction)[25] <- "Notes"

human <- subset(human_extraction, select = -c(Human, Notes))
human <- human %>% mutate(Citation = substr(Citation, 1,35))

human_long <- human  %>% 
  pivot_longer(cols = -Citation, 
               names_to = "Question",
               values_to = "Answer") %>% 
  mutate(Source="human") %>%
  select(Citation, Question, Source, Answer)

GPT4x1_long <- GPT4x1 %>% 
  pivot_longer(cols = -Citation, 
               names_to = "Question",
               values_to = "Answer") %>% 
  mutate(Source="GPT4x1") %>%
  select(Citation, Question, Source, Answer)

GPT4x3_long <- GPT4x3  %>% 
  pivot_longer(cols = -Citation, 
               names_to = "Question",
               values_to = "Answer") %>% 
  mutate(Source="GPT4x3") %>%  
  select(Citation, Question, Source, Answer)

Elicit_long <- Elicit  %>% 
  pivot_longer(cols = -Citation, 
               names_to = "Question",
               values_to = "Answer") %>% 
  mutate(Source="Elicit") %>%  
  select(Citation, Question, Source, Answer)

fulldf <- bind_rows(human_long, GPT4x1_long, GPT4x3_long, Elicit_long)
widedf <- fulldf %>% pivot_wider(names_from = 'Source', values_from = 'Answer')

widedf[] <- lapply(widedf, trimws)
df <- widedf %>% 
  mutate_all(~ifelse(. == "NA", NA, .)) %>% 
  mutate_all(~ifelse(. == "NA***", NA, .)) %>% 
  mutate_all(~ifelse(. == "NA; NA; NA", NA, .)) %>% 
  mutate_all(~ifelse(. == "NO DATA", NA, .)) %>%
  mutate_all(~ifelse(. == "NO CONTEXT;  NO CONTEXT;  NO CONTEXT", NA, .)) %>% 
  mutate_all(~ifelse(. == "NO CONTEXT;  NO CONTEXT; NO DATA", NA, .)) %>%
  mutate_all(~ifelse(. == "NO DATA;  NO CONTEXT;  NO CONTEXT", NA, .))
newdf <- df %>% mutate(Category = ifelse(grepl("Response", Question,), "response", "context"))

pal = c("#a6611a",
        "#dfc27d",
        "#80cdc1",
        "#018571")

(newdfx <- newdf %>% pivot_longer(c(GPT4x1, GPT4x3, Elicit), names_to = "AI", values_to = "value") %>% 
    mutate(human_data = ifelse(is.na(human),"No Data","Data"),
           ai_data = ifelse(is.na(value),"No Data","Data")) %>% 
    mutate(human_data = factor(human_data,levels = c('Data',"No Data")),
           ai_data = factor(ai_data, levels = c("Data","No Data") %>% rev)) %>% 
    group_by(AI,human_data,ai_data,Category) %>% 
    summarise(value =  signif(n()/363, digits = 2)) %>% as_tibble() %>% 
    complete(AI,human_data,ai_data,Category, fill = list(value = 0)) %>% 
    ungroup() %>% 
    mutate(display_value = ifelse(value >= 0.01 | value == 0, value,"<0.01")) %>% 
  ggplot() +
  geom_tile(aes(x = human_data, y = ai_data, fill = interaction(human_data,ai_data))) +
  geom_text(aes(x = human_data, y = ai_data, label = display_value)) +
  xlab("Human") +
  ylab("AI") +
   scale_fill_manual(values = pal[c(1,4,4,2)]) +
  facet_wrap(Category ~ AI) +
  theme_classic() +
  theme(legend.position = "none"))

# ggsave("Figures/confusion_matrix.png", width = 7, height = 5, device = "png")
```




## OLD stats and analysis - IGNORE pretty much everything in here
### things in this section may not be super relevant - may be old stats and old values

### Effect of question difficulty on assessment value
```{r correlation test, echo = FALSE}
diffdata <- paper_df %>% filter(group_agent != "GPT4-Turbo Assessor")
write.csv(diffdata, "diffdata.csv")
cor.test <- cor.test(diffdata$mean, diffdata$difficulty_mean)
print(cor.test)
```
Data shows that p is 0.48, therefore not significant and there is no relationship between difficulty of questions for each paper and the overall assessment value. This can be seen for each AI model in the plot below.


Also used a linear mixed model to determine if the assessment mean is affected by the relationship between the difficulty and the AI model, with the citation being a random effect
```{r diffeffect, echo = TRUE}
diff <- lmer(value ~ Difficulty*AI + (1|Citation) + (1|Extractor) + (1|Reviewer), data = statdfx)
anova(diff)
summary(diff)
# diff_emmeans <- emmeans(diff, ~AI)
# diff_cont <- pairs(diff_emmeans, adjust = "tukey")
# print(diff_cont)

diffdata <- question_df %>% filter(Reviewer != "GPT4-Turbo")

# diffeffect <- lmer(mean ~ difficulty_mean*AI + (1|Question) + (1|Reviewer), data = diffdata) #better than null
# summ <- summary(diffeffect)
# anova(diffeffect)
# diffeffect_emmeans <- emmeans(diffeffect, ~AI)
# diffeffect_cont <- pairs(diffeffect_emmeans, adjust = "tukey")
# print(diffeffect_cont)
# 
# diff_ai <- lmer(mean ~difficulty_mean + (1|Citation) + (1|Extractor), data = filter(diffdata, AI == "GPT4x3"))
# anova(diff_ai)
# 
# diff_null <- lmer(mean ~ difficulty_mean + (1|Citation) + (1|Extractor), data = diffdata)
# summ2 <- summary(diff_null)
# anova(diff_null)
# 
# diffanova <- anova(diff_null, diffeffect)
# print(diffanova)
```
The lower AIC and BIC values indicate that diffeffect (including AI as a fixed variable in the model) has a better fit, and is significantly different from diff_null. Therefore, it does appear that the difficulty of the question/citation does affected the assessment value differently by the AI models (I think)...


