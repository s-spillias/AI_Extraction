# Flagged

## Flagged for "Which country was the study conducted in?"

> Human response: "Response: 2. Solomon Islands 3. Vanautu:"
Was a human response omitted here (e.g., 1. Fiji)?

> Human response: "Response: 1. Samoa 2. Cook Islands 3. Fiji 4. Palau 5. Tuvalu"
Here, the AI pulled out things like Hawaii. Thought I better flag for you to check what's gone down here.

> AI response: "Solomon Islands and Shetlands Islands"
Context for both AI and humans points suggests that AI is (partly) correct and the humans failed to mention Scottland.

## Flagged for "Provide some background as to the drivers and/or motivators of community-based fisheries management"

- At least once, humans provided context without a response (I zoomed out to make sure it was not cut off). I forgot to flag this, but I think it was the "_Small-scale managed marine areas over time Develo.pdf" paper

- There was a human response where five drivers were listed, but the fourth and ftih were listed as: "4. Indiscernible 5. Indiscernible"

> Humans response begun with "Food supply..."
Here, the AI mentions the influence of global trends of "social justice, environmental sustainability, and an increasing recognition of indigenous governance practices". This is not mentioned by the humans at all. Same with mentioning "COVID-19". As far as I can tell, this is not in the context either. I have no clue if this conclusion is warranted by the paper. It might be worth following up if the AI is talking nonsense or if there is missing context.

> Humans response was "Existing Custom linking fishers to place"
Here, the AI's context is "see the contents provided". I flagged this because the AI's context was particularly bad.

# Flagged for "How was the data on benefits collected?"

- AI response mentions "catch per unit effort" when the human and other AI do not mention this. There is some provided context, but it's hard to assess the relevance.

- Sometimes humans did not provide a response but AI did. Might be worth checking AI response is correct. I've considered commitment to conservation rules to be a benefit, perhaps the human raters did not.

- Sometimes the AI gave a "nan" context but provided a response

- I flagged a few responses where there is high-agreement between humans and AI (both claim no data was collected). But, the AI provides context and a response. I'm unsure how to code this response, so I've responded with 3's on everything. In some of these cases, the AI was less clear, and received small scores.

> Human response: "surveys, assessments"
- I flagged this because the human response ("assessments") is somewhat lacking.

> Human response: "interviews, ranking"
- Flagged because no context was provided by humans

# Notes

For "Provide some background as to the drivers and/or motivators of community-based fisheries management", I think this broad question resulted in broad responses from both humans and AI. In my experience, rarely AI and human responses were similar, AI tended to be far broader, and the contexts often differed. For this item, "AI2Human" might be the most diagnostic measure of performance (assuming humans are stating the correct answer), with little information offered by "Context2Question" and "Response2Context".