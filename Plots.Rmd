---
title: "plots"
output: html_document
date: "2024-07-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include=FALSE}
library(knitr)
library(dplyr)
library(tidyverse)
library(readxl)
library(beyonce)
library(ggpubr)
library(lme4)
library(lmerTest)
options(max.print = 999999)

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# open files - create dataframes #
file_path = 'output_assessment_CHATGPT_explanation.xlsx'
validate_path = list.files('Raw_Team_Assessment', pattern = 'assessment', full.names = TRUE)
figure_folder = '../Manuscript/Figures'
# firstcontact-gpt4-32k
# January 29, ran Elicit and GPT4-32k
sheet_names <- excel_sheets(file_path)

all_data_list <- lapply(sheet_names, function(sheet) {
  read_excel(file_path, sheet = sheet) %>% 
    mutate(AI = sheet)})

do_stats = function(x){
  x %>% 
    summarise(mean = mean(value),
              standard_error = sd(value, na.rm = TRUE) / sqrt(n()),
              st_dev = sd(value),
              difficulty_mean = mean(Difficulty, na.rm = TRUE),
              st_dev_difficulty = sd(Difficulty, na.rm = TRUE),
              standard_error_difficulty = sd(Difficulty, na.rm = TRUE) / sqrt(n()),
    )}

difficulty <- read_excel("Extraction_Human_Difficulty.xlsx") %>% as_tibble() %>% 
  pivot_longer(-Question) %>% 
  setNames(c('Citation','Question','Difficulty')) %>% 
  mutate(Difficulty = case_match(Difficulty, 'Easy' ~ 1, 'Medium' ~ 2, 'Hard' ~ 3))

file_path = 'Extraction_ai.xlsx'
sheets = excel_sheets(file_path)
ai_extractions <- sheets %>% lapply(function(x) read_excel(file_path, sheet = x) %>% as_tibble() %>% 
                                      mutate(across(everything(), function(x) x %>% str_replace("Response: ",""))) %>%
                                      separate_wider_delim(-Citation, delim = "Context:", names = c('Response','Context'), names_sep = '***',too_few = "align_start") %>% 
                                      mutate(across(-Citation, nchar))) %>% setNames(sheets)

human_extraction = "Human_Extraction_Cleaned.xlsx" %>% 
  read_excel(sheet = 'Human') %>%
  select(-5, -(seq(8, ncol(.), by = 3))) %>%
  select(-1) %>% 
  slice(-(1:2)) %>% 
  mutate(across(-Question, nchar)) %>% 
  select(-last_col())

verbosity_ratios = sheets %>% lapply(function(x) bind_cols(human_extraction$Question, (ai_extractions[[x]] %>% select(-1))/(human_extraction %>% select(-1))))

reviewers = c(
  'R1', "Which country was the study conducted in?",
  'R1',"Provide some background as to the drivers and/or motivators of community-based fisheries management.",
  'R2',"What management mechanisms are used?",
  'R3',"Which groups of people are involved in the management as part of the CBFM case-studies? Choices: Community-members, Researchers, Practitioners, Government, NGOs, Other",
  'R4',"What benefits of Community-Based Fisheries Management are reported in this case study?",
  'R4',"What are the indicators of success of CBFM?",
  'R1',"How was the data on benefits collected?",
  'R3',"What are the reported barriers to success of Community-Based Fisheries Management?",
  'R2',"Guidelines for future implementation of CBFM?",
  'R3',"How does the community monitor the system they are managing?",
  'R4',"How does the community make decisions?")

extractor <- "Human_Extraction_Cleaned.xlsx" %>% 
  read_excel(sheet = 'Human') %>%
  dplyr::select(-5, -(seq(8, ncol(.), by = 3))) %>%
  #dplyr::select(-1) %>% 
  dplyr::slice(-(1:2)) %>% 
  mutate(across(-Question, nchar)) %>% 
  dplyr::select(-last_col()) %>% 
  select(-3:-24) %>% 
  mutate(short_cit = substr(Question, 1,35)) #no idea why citation column is actually the question column in this instance
colnames(extractor)[1] <- "Extractor"
colnames(extractor)[2] <- "Citation"
colnames(extractor)[3] <- "short_cit"

reviewers_id <- matrix(reviewers, nrow = length(reviewers) / 2, byrow = TRUE) %>% as_tibble() %>% 
  setNames(c("Reviewer","Question"))

df_description = all_data_list %>% bind_rows() %>% 
  pivot_longer(-c('Citation','AI')) %>% 
  separate(value, sep = ':::', into = c("value", "description")) %>% 
  separate(value, sep = "\\|", into = paste0("rep",1:5)) %>% 
  separate(description, sep = "\\|", into = paste0("Description",1:5)) %>% 
  pivot_longer(starts_with('rep'), names_to = 'Agent') 

ai_df <- df_description %>% 
  dplyr::select(-starts_with('Description')) %>% 
  separate(value, sep = ';', into = paste0('Crit', 1:4)) %>% 
  pivot_longer(starts_with('Crit'), names_to = 'Criteria')

team_path = "output_assessment_TEAM.xlsx" #original file with unedited flag values = output_assessment_TEAM_OG.xlsx

sheet_names <- excel_sheets(team_path)

team_data_list <- lapply(sheet_names, function(sheet) {
  read_excel(team_path, sheet = sheet) %>% 
    mutate(AI = sheet)
})
# 
# df_validate = validate_path %>% 
#   lapply(function(x) sheet_names %>% lapply(function(y)    read_excel(x, sheet = y) %>% 
#                                               mutate(AI = y, Reviewer = x %>% str_remove("Raw_Team_Assessment/validate_assessment_") %>% str_remove('.xlsx')) %>% 
#                                               bind_rows())
#   ) %>% bind_rows() %>% 
#   pivot_longer(-c('Citation','AI','Reviewer'), names_to = 'Question',values_to = 'Value') %>% 
#   filter(!(Value %>% str_detect("Response|Context"))) %>% 
#   pivot_wider(names_from = 'Reviewer', values_from = 'Value') %>% 
#   filter(!is.na(R1)) %>% 
#   pivot_longer(-c('Citation','AI','Question'), names_to = 'Reviewer',values_to = 'Value') %>% 
#   separate(Value, sep = ';', into = paste0('Crit',1:4)) %>% 
#   pivot_longer(starts_with('crit'),values_to = 'value', names_to = 'Criteria') %>% 
#   mutate(value = as.numeric(value)) %>% 
#   # mutate(Reviewer = recode(Reviewer, 'MATT' = 'R1',
#   #                          'FABIO' = 'R2',
#   #                          'SCOTT' = 'R3',
#   #                          'ROWAN' = 'R4'))

stat_summary <- team_data_list %>% bind_rows() %>% 
  pivot_longer(-c("Citation","AI")) %>% 
  separate(value, sep = ';', into = paste0('Crit',1:4)) %>% 
  pivot_longer(starts_with("Crit"), names_to = 'Criteria') %>% 
  mutate(Agent = 'Human') %>% 
  bind_rows(ai_df) %>% 
  mutate(Flag = ifelse(Criteria == 'Crit4' & value > 1, "FLAG",NA),
         group_agent = ifelse(str_detect(Agent, "Human"), "Human Assessor", "GPT4-Turbo Assessor"),
         value = as.integer(value)) %>% 
  {flag_df <<- .} %>% 
  filter(!is.na(value)) %>% 
  filter(Criteria != 'Crit4') %>%
  relocate(value, .after = 'group_agent') %>% 
  mutate(Criteria = case_match(Criteria, 'Crit1' ~ 'Context to Question',
                               'Crit2' ~ 'Response to Context',
                               'Crit3' ~ 'AI Response to Human Response')) %>% 
  rename('Question' = 'name') %>% 
  left_join(reviewers_id, by = 'Question') %>% 
  mutate(Reviewer = ifelse(str_detect(group_agent,'GPT'),'GPT4-Turbo',Reviewer)) %>% 
  mutate(short_cit = substr(Citation, 1,35)) %>% 
  left_join(difficulty %>%  mutate(short_cit = substr(Citation, 1,35)) %>% dplyr::select(-Citation), by = c("Question",'short_cit')) %>%
  mutate(AI = case_match(AI, 'ELICIT' ~ 'Elicit',
                         'OPENAI' ~ 'GPT4x3',
                         'OPENAI_single' ~ 'GPT4x1')) %>% 
  
  mutate(Question = factor(str_split_fixed(Question, '\\?',2)[,1],levels = str_split_fixed(reviewers_id$Question, '\\?',2)[,1])) %>% 
  {full_df <<- .} %>% 
  group_by(group_agent, AI,Criteria) %>% 
  do_stats()

full_df$value <- full_df$value - 2
stat_summary$mean <- stat_summary$mean - 2

full_df <- full_df %>%
  left_join(extractor, by = "short_cit") %>%
  mutate(Extractor = ifelse(group_agent == "Human Assessor", Extractor, "AI")) %>% 
  select(-Citation.y) %>% 
  rename(Citation = Citation.x)

question_df <- full_df %>% 
  group_by(Reviewer, AI, Criteria, Question) %>% 
  do_stats()

paper_df <- full_df %>% 
  group_by(group_agent, AI, Criteria, Citation) %>% 
  do_stats()

reviewer_df <- full_df %>% 
  group_by(Reviewer, Criteria) %>% 
  do_stats()

flag_df <- flag_df %>% filter(!is.na(Flag))



### Make Plots ####
crit_pal = beyonce_palette(127)[-1]
# ## Overall
# ggplot(stat_summary, aes(x = AI, y = mean, fill = Criteria, group = Criteria)) +
#   geom_hline(yintercept = 2, linetype = 'dashed',alpha = 0.5 ) +
#   geom_col(stat = "identity", position = "dodge", color = "black") +
#   geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
#                 position = position_dodge(width = 0.9), width = 0.25) +
#   labs( x = "AI Implementation", y = "Assessed Quality") +
#   theme_classic() +
#   coord_cartesian(ylim = c(1, 3)) +
#   facet_wrap(~group_agent) +
#   #scale_y_continuous(    labels = c("Poor","Fair","Good"),breaks = c(1,2,3),) +
#   scale_fill_manual(values = crit_pal) +
#   theme(legend.position = 'bottom') 
# #ggsave(file.path(figure_folder,'overall.png'))
# 
# 
# ## Questions
# question_df %>% filter(!str_detect(Reviewer,'GPT')) %>% ungroup() %>% 
#   mutate(
#     normalized_difficulty_mean = 1 + 2*((difficulty_mean - min(difficulty_mean,na.rm = TRUE)) / (max(difficulty_mean,na.rm = TRUE) - min(difficulty_mean,na.rm = TRUE))),
#     normalized_difficulty_st_dev = st_dev_difficulty / (max(st_dev_difficulty) - min(st_dev_difficulty)),
#     normalized_difficulty_standard_error = standard_error_difficulty / (max(standard_error_difficulty) - min(standard_error_difficulty))
#   ) %>% 
#   arrange(difficulty_mean) %>% ungroup() %>% 
#   ggplot() +
#   aes(x = AI, y = mean, fill = Criteria, group = Criteria) +
#   geom_hline(yintercept = 2, linetype = 'dashed',alpha = 0.5 ) +
#   geom_col(stat = "identity", position = "dodge", color = "black") +
#   geom_segment(aes(x = 4, y = 1.1, xend = 4, yend = 2.9), col = 'orange') +
#   geom_point(aes(x = 4,y = normalized_difficulty_mean), size = 3, col = 'orange', show.legend = FALSE) +
#   geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
#                 position = position_dodge(width = 0.9), width = 0.25) +
#   # labs(title = "Mean Values by Group and Criteria", x = "Group", y = "Mean Value") +
#   theme_classic() +
#   coord_cartesian(ylim = c(1, 3), xlim = c(1,3.7)) +
#   facet_wrap(~Question, labeller = label_wrap_gen()) +
#   labs(x = '') +
#   scale_fill_manual(values = crit_pal) +
#   scale_y_continuous(
#     name = "Assessed Quality",
#     #labels = c("Poor","Fair","Good"),breaks = c(1,2,3),
#     sec.axis = sec_axis(~ (. - 1)/2, name = "Question Difficulty", labels = c("Easy","Hard"), breaks = c(0,1)),
#   ) +
#   theme(axis.text.y.right = element_text(color = "orange"),
#         strip.text = element_text(size = 8),
#         legend.position = 'bottom',
#   ) +
#   geom_text(aes(x = 4.2, y = 1.15, label = Reviewer, hjust = 1, vjust = 1), size = 3) 
# #ggsave(file.path(figure_folder,'questions_supp.png'), width = 12, height = 7)
# 
# #Check for Reviewer Consistency and validation
# ggplot(reviewer_df, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
#   geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
#   geom_col(stat = "identity", position = "dodge", color = "black") +
#   geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
#                 position = position_dodge(width = 0.9), width = 0.25) +
#   labs(x = "Reviewer", y = "Mean Assessed Quality") +
#   theme_classic() +
#   coord_cartesian(ylim = c(1, 3)) +
#   theme(legend.position = 'bottom') +
#   #facet_wrap(~Question) +
#   scale_fill_manual(values = crit_pal)  # Set your desired fill colors
# #ggsave(file.path(figure_folder,'reviewer_supp.png'), width = 12, height = 7)
# 
# df_validate_stats = df_validate %>%
#   filter(Criteria !='Crit4') %>% 
#   group_by(Criteria, Reviewer) %>% 
#   summarise(mean = mean(value),
#             standard_error = sd(value, na.rm = TRUE) / sqrt(n()),
#             st_dev = sd(value))
# ggplot(df_validate_stats, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
#   geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
#   geom_col(stat = "identity", position = "dodge", color = "black") +
#   geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
#                 position = position_dodge(width = 0.9), width = 0.25) +
#   labs(x = "Reviewer", y = "Mean Assessed Quality") +
#   theme_classic() +
#   coord_cartesian(ylim = c(1, 3)) +
#   theme(legend.position = 'bottom') +
#   #facet_wrap(~Question) +
#   scale_fill_manual(values = crit_pal)
# 
# df_validate$value <- df_validate$value-2
# 
# df_validate %>% filter(Criteria != 'Crit4') %>% ggplot() +
#   geom_bar(stat = 'identity', aes(x = Criteria, y = value, fill = Reviewer), position = 'dodge') +
#   facet_wrap(AI~Question)

full_df %>% filter(!str_detect(Reviewer,'GPT')) %>% 
  group_by(Reviewer,Criteria) %>% 
  ggplot() +
  geom_density(fill = 'skyblue',aes(x = value), adjust = 2.5) +
  facet_grid(Reviewer~ Criteria) +
  theme_classic()

##Impact of Difficulty
ggplot(question_df %>% filter(!str_detect(Reviewer,'GPT')), aes(x = difficulty_mean, y = mean, col = Criteria)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +  # Use method = "lm" for linear model
  #stat_poly_line() +
  #stat_poly_eq() +
  theme_classic() +
  facet_wrap(~AI) +
  labs(y = 'Mean Assessed Quality', x = 'Mean Question Difficulty') +
  
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = crit_pal)
  #ggsave(file.path(figure_folder,'question_difficulty_supp.png'), width = 12, height = 7)


ggplot(paper_df %>% filter(str_detect(group_agent,'Human')), 
       aes(x = difficulty_mean, y = mean, col = AI)) +
  geom_point() +
  labs(y = 'Mean Assessed Quality', x = 'Mean Paper Difficulty') +
  
  geom_smooth(method = "lm", se = FALSE) +  # Use method = "lm" for linear model
  theme_classic() +
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = crit_pal) # Set your desired fill colors
  #ggsave(file.path(figure_folder,'paper_difficulty_supp.png'), width = 12, height = 7)

#### analysis ####
# combining median value for the 5 replicates of the AI assessor
humandf <- full_df %>% filter(Agent == "Human") %>% select(-short_cit)
aidf <- full_df %>% filter(Agent != "Human")
aidf2 <- aidf %>% group_by(Citation, AI, Question, Criteria, Flag, group_agent, Reviewer, Difficulty, Extractor) %>% 
  summarise(value = median(value)) %>% mutate(Agent = "AI") %>% 
  select(Citation, AI, Question, Criteria, Agent, Flag, group_agent, value, Reviewer, Difficulty, Extractor)
statdf <- rbind(humandf, aidf2)

# anova of this - may be INVALID due to distribution of data
aiaov <- aov(data = statdf, value ~ AI)
summary(aiaov)
tt <- TukeyHSD(aiaov)
tt
plot(aiaov)


# validating reviewers #
statdf2 <- statdf %>% filter(Reviewer != "GPT4-Turbo")
val <- aov( data = statdf2, value ~ Reviewer*Criteria)
summary(val)
valtt <- TukeyHSD(val)
valtt
# 
# df_validate_stats <- df_validate_stats %>% mutate(Criteria = case_match(Criteria, 'Crit1' ~ 'Context to Question',
#                                                                         'Crit2' ~ 'Response to Context',
#                                                                         'Crit3' ~ 'AI Response to Human Response'))
# valplot <- {ggplot(df_validate_stats, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
#     geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
#     geom_col(stat = "identity", position = "dodge", color = "black") +
#     geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
#                   position = position_dodge(width = 0.9), width = 0.25) +
#     labs(x = "Reviewer", y = "Mean Assessed Quality") +
#     theme_classic() +
#     coord_cartesian(ylim = c(1, 3)) +
#     theme(legend.position = 'bottom') +
#     #facet_wrap(~Question) +
#     scale_fill_manual(values = crit_pal)} #will need to run process_evaluations_new before to get objects
# 
# df_val <- df_validate %>% filter(Criteria != "Crit4") %>% mutate(Criteria = case_match(Criteria, 'Crit1' ~ 'Context to Question',
#                                                                                        'Crit2' ~ 'Response to Context',
#                                                                                        'Crit3' ~ 'AI Response to Human Response'))
# crit1_mean <- df_val %>% filter(Criteria == "Context to Question") # crit 1 - in red
# crit1_mean <- mean(crit1_mean$value)
# print(crit1_mean) #mean = 2.458
# 
# crit2_mean <- df_val %>% filter(Criteria == "Response to Context") #crit 2 - in green
# crit2_mean <- mean(crit2_mean$value)
# print(crit2_mean) #mean = 2.533
# 
# crit3_mean <- df_val %>% filter(Criteria == "AI Response to Human Response") #crit 3 - in blue
# crit3_mean <- mean(crit3_mean$value)
# print(crit3_mean) #mean = 2.242
# 
# # attempting to create 5th "reviewer" which is all combined data - I think is valid (talk with Dave) but not best approach
# df_val2 <- df_val %>% filter(Criteria != "NA") %>% mutate(Reviewer = "R5")
# valbind <- rbind(df_val, df_val2)
# 
# valbind_stats = valbind %>% 
#   group_by(Criteria, Reviewer) %>% 
#   summarise(mean = mean(value),
#             standard_error = sd(value, na.rm = TRUE) / sqrt(n()),
#             st_dev = sd(value))
# valplot2 <- {ggplot(valbind_stats, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
#     geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
#     geom_col(stat = "identity", position = "dodge", color = "black") +
#     geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
#                   position = position_dodge(width = 0.9), width = 0.25) +
#     labs(x = "Reviewer", y = "Mean Assessed Quality") +
#     theme_classic() +
#     coord_cartesian(ylim = c(1, 3)) +
#     theme(legend.position = 'bottom') +
#     #facet_wrap(~Question) +
#     scale_fill_manual(values = crit_pal)} # compares individual reviewers (1-4) to the mean of all the values (R5)
# 
# valaov <- aov(data = valbind, value ~ Reviewer*Criteria)
# summary(valaov)
# val_hsd <- TukeyHSD(valaov)
# val_hsd 
# #only reviewer 1 is sig. diff. from "reviewer 5" (the average of all data) WHEN ALL CRITERIA COMBINED
# #no interaction between reviewer*criteria (p = 0.222) - only sig. diff is R1 vs R3 response to context

## LMM analysis ##
library(lme4)
lmm_model <- lmer(value ~ AI + (1|Reviewer), data = statdf)
summary(lmm_model)
lmm_mod_anova <- anova(lmm_model)
#TukeyHSD(lmm_mod_anova)

## GLMM analysis ##
library(lme4)
library(emmeans)

statdfx <- statdf %>% filter(Agent == "Human")

# glmm_mod <- glmer(value ~ AI + (1|Reviewer), data = statdfx, family = poisson) #but is poisson legit? only one that works
# summary(glmm_mod)
# anova(glmm_mod)
# emm <- emmeans(glmm_mod, ~ AI)
# contrasts <- pairs(emm, adjust = "tukey")
# print(contrasts)
# 
# glmm_mod2 <- glmer(value ~ AI*Criteria + (1|Reviewer), data = statdf, family = poisson) #but is poisson legit? only one that works
# summary(glmm_mod2)
# anova(glmm_mod2)
# emm2 <- emmeans(glmm_mod2, ~ AI*Criteria)
# contrasts2 <- pairs(emm2, adjust = "tukey")
# print(contrasts2)
# 
# glmm_mod <- glmer(value ~ group_agent*AI + (1|Reviewer), data = statdf, family = poisson) #but is poisson legit? only one that works
# summary(glmm_mod)
# anova(glmm_mod)
# emm <- emmeans(glmm_mod, ~ group_agent*AI)
# contrasts <- pairs(emm, adjust = "tukey")
# print(contrasts)
# 
# mod_glmm <- glmer(value ~ AI*group_agent + (1|Reviewer), data = statdf, family = poisson)
# summary(mod_glmm)
# anova(mod_glmm)
# emm_glmm <- emmeans(mod_glmm, ~ AI|group_agent)
# pairs(emm_glmm)
# emm_int_glmm <- emmeans(mod_glmm, pairwise ~ AI:group_agent)
# pairs(emm_int_glmm)
# 
# # comparing full model with interaction to reduced model without - if p < 0.05, then interaction between AI and group_agent is sig.
# mod_red <- glmer(value ~ AI + group_agent + (1|Reviewer), data = statdf, family = poisson)
# anova_res <- anova(mod_red, mod_glmm)
# print(anova_res)
# 
# #### secondary analysis ####
# statdfx <- statdf %>% filter(Agent == "Human")
# xx <- glmer(value ~ AI + (1|Reviewer), family = poisson, data = filter(statdfx, Criteria == "AI Response to Human Response"))
# 
# #only comparing the value to the intercept with reviewer as the random effect 
# attempt1 <- lmer(value ~ 1 + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
# summary(attempt1)
# randoms <- ranef(attempt1, condVar = TRUE) #determining intercepts
# 
# #comparing the value by each AI (reviewer still random)
# attempt2 <- lmer(value ~ AI + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
# summary(attempt2)
# att2 <- emmeans(attempt2, ~ AI)
# contrasts <- pairs(att2, adjust = "tukey")
# print(contrasts)
# 
# #anova comparing the two models
# anova(attempt1, attempt2) #AIC lower for attempt 2, better, meaning AI has an effect
# 
# #now including question as a random effect
# attempt3 <- lmer(value ~ AI + (1|Question) + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
# anova(attempt2, attempt3)
# summary(attempt3)

#### PROPER MODEL TESTS ####
statdfx <- statdf %>% filter(Agent == "Human") #only investigating human assessors (no group agent)

#comparing distribution of values to the mean of 2
statdfx %>% group_by(AI, Criteria) %>% summarise(t_value = t.test(value, mu = 2, alternative = "less")$statistic)
statdfx %>%  group_by(AI, Criteria) %>%  summarise(t_value = t.test(value, mu = 2, alternative = "less")$statistic, p_value = t.test(value, mu = 2, alternative = "less")$p.value)
output1 <- statdfx %>%  group_by(AI, Criteria) %>%  summarise(t_value = t.test(value, mu = 2, alternative = "less")$statistic, p_value = t.test(value, mu = 2, alternative = "less")$p.value)
ungroup <- ungroup(output1)

#effect of difficulty on values for AI response to human response
diffdata <- paper_df %>% filter(group_agent != "GPT4-Turbo Assessor") %>% mutate(short_cit = substr(Citation, 1,35))
diffdata <- diffdata %>% left_join(select(extractor, short_cit, Extractor), by = c("short_cit")) 
diffdata <- subset(diffdata, select = -short_cit)
write.csv(diffdata, "diffdata.csv")
cor.test(diffdata$mean, diffdata$difficulty_mean) #correlation test to see if mean and difficulty mean are related (p = 0.48 - not sig.)

diffeffect <- lmer(mean ~ difficulty_mean + AI + (1|Citation), data = diffdata)
summ <- summary(diffeffect)
anova(diffeffect)

diff_null <- lmer(mean ~ difficulty_mean + (1|Citation), data = diffdata)
summary(diff_null)
anova(diff_null)

anova(diff_null, diffeffect)

diff_plot <- ggplot(diffdata, aes(x = difficulty_mean, y = mean, color = AI, shape = AI)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Difficulty Mean", y = "Mean", color = "AI Model", shape = "AI Model") +
  ggtitle("Relationship between Mean and Difficulty Mean by AI Model")

#effect of AI on values for AI response to human response
aieffect <- lmer(value ~ AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "Context to Question"))
summary(aieffect)
aieffect_emmeans <- emmeans(aieffect, ~ AI)
aieffect_cont <- pairs(aieffect_emmeans, adjust = "tukey")
print_aieffect <- print(aieffect_cont)

#effect of question on values for AI response to human response
qeffect <- lmer(value ~ Question + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
summary(qeffect)


```

### Key Plots
```{r}
library(tidyverse)
library(readxl)
library(dplyr)
library(xlsx)

#### set-up ####
file_path = 'Extraction_ai.xlsx'
sheets = excel_sheets(file_path)
ai_extractions <- sheets %>% lapply(function(x) read_excel(file_path, sheet = x) %>% as_tibble() %>% 
    mutate(across(everything(), function(x) x %>% str_replace("Response: ",""))) %>%
    separate_wider_delim(-Citation, delim = "Context: ", names = c('Response','Context'), names_sep = '***',too_few = "align_start")) %>% 
  setNames(sheets)

GPT4x1 <- ai_extractions[['OPENAI_single']] %>% mutate(Citation = substr(Citation, 1,35))
GPT4x3 <- ai_extractions[['OPENAI']] %>% mutate(Citation = substr(Citation, 1,35))
Elicit <- ai_extractions[['ELICIT']] %>% mutate(Citation = substr(Citation, 1,35))

human_extraction = "Human_Extraction_Cleaned.xlsx" %>% 
  read_excel(sheet = 'Human') %>%
  dplyr::select(-5, -(seq(8, ncol(.), by = 3))) %>%
  slice(-(1:2))
human_extraction <- human_extraction[,c(2:24, 1, 25)]
names(human_extraction) <- names(GPT4x1)
colnames(human_extraction)[24] <- "Human"
colnames(human_extraction)[25] <- "Notes"

human <- subset(human_extraction, select = -c(Human, Notes))
human <- human %>% mutate(Citation = substr(Citation, 1,35))

#### long form and merging data ####
human_long <- human  %>% 
  pivot_longer(cols = -Citation, 
               names_to = "Question",
               values_to = "Answer") %>% 
  mutate(Source="human") %>%
  select(Citation, Question, Source, Answer)

GPT4_long <- GPT4x1 %>% 
  pivot_longer(cols = -Citation, 
               names_to = "Question",
               values_to = "Answer") %>% 
  mutate(Source="GPT4") %>%
  select(Citation, Question, Source, Answer)

GPT4x3_long <- GPT4x3  %>% 
  pivot_longer(cols = -Citation, 
               names_to = "Question",
               values_to = "Answer") %>% 
  mutate(Source="GPT4x3") %>%  
  select(Citation, Question, Source, Answer)

Elicit_long <- Elicit  %>% 
  pivot_longer(cols = -Citation, 
               names_to = "Question",
               values_to = "Answer") %>% 
  mutate(Source="Elicit") %>%  
  select(Citation, Question, Source, Answer)

fulldf <- bind_rows(human_long, GPT4_long, GPT4x3_long, Elicit_long)
widedf <- fulldf %>% pivot_wider(names_from = 'Source', values_from = 'Answer')


#### comparing NA values ####
widedf[] <- lapply(widedf, trimws)
df <- widedf %>% 
  mutate_all(~ifelse(. == "NA", NA, .)) %>% 
  mutate_all(~ifelse(. == "NA***", NA, .)) %>% 
  mutate_all(~ifelse(. == "NA; NA; NA", NA, .)) %>% 
  mutate_all(~ifelse(. == "NO DATA", NA, .)) %>%
  mutate_all(~ifelse(. == "NO CONTEXT;  NO CONTEXT;  NO CONTEXT", NA, .)) %>% 
  mutate_all(~ifelse(. == "NO CONTEXT;  NO CONTEXT; NO DATA", NA, .)) %>%
  mutate_all(~ifelse(. == "NO DATA;  NO CONTEXT;  NO CONTEXT", NA, .))

r.hum_vs_GPT4x1<- df %>% filter(is.na(human) & !is.na(GPT4)) #110 instances where GPT4x1had response and human responded with NA
r.GPT4_vs_hum <- df %>% filter(is.na(GPT4) & !is.na(human)) #154 instances where human had response and GPT4x1responded with NA

r.hum_vs_GPT4x3 <- df %>% filter(is.na(human) & !is.na(GPT4x3)) #137 instances where GPT4x3 had response and human responded with NA
r.GPT4x3_vs_hum <- df %>% filter(is.na(GPT4x3) & !is.na(human)) #111 instances where human had response and GPT4x3 responded with NA

r.hum_vs_Elicit <- df %>% filter(is.na(human) & !is.na(Elicit)) #191 instances where Elicit had response and human responded with NA
r.Elicit_vs_hum <- df %>% filter(is.na(Elicit) & !is.na(human)) #1 instance where human had response and Elicit responded with NA


#### separate response/context ####
newdf <- df %>% mutate(Category = ifelse(grepl("Response", Question,), "response", "context"))
response <- newdf %>% filter(Category == "response")
context <- newdf %>% filter(Category == "context")

## for responses ##
r.hum_vs_GPT4x1<- response %>% filter(is.na(human) & !is.na(GPT4)) #54 instances where GPT4x1had response and human responded with NA
r.GPT4_vs_hum <- response %>% filter(is.na(GPT4) & !is.na(human)) #76 instances where human had response and GPT4x1responded with NA

r.hum_vs_GPT4x3 <- response %>% filter(is.na(human) & !is.na(GPT4x3)) #68 instances where GPT4x3 had response and human responded with NA
r.GPT4x3_vs_hum <- response %>% filter(is.na(GPT4x3) & !is.na(human)) #55 instances where human had response and GPT4x3 responded with NA

r.hum_vs_Elicit <- response %>% filter(is.na(human) & !is.na(Elicit)) #94 instances where Elicit had response and human responded with NA
r.Elicit_vs_hum <- response %>% filter(is.na(Elicit) & !is.na(human)) #0 instance where human had response and Elicit responded with NA

r.na.humGPT4x1<- response %>% filter(is.na(human) & is.na(GPT4)) #40 instances where both returned NA
r.na.humgtp4x3 <- response %>% filter(is.na(human) & is.na(GPT4x3)) #26 instances where both returned NA
r.na.humelicit <- response %>% filter(is.na(human) & is.na(Elicit)) #0 instances where both returned NA

r.ans.humgtp4 <- response %>% filter(!is.na(human) & !is.na(GPT4)) #193 instances where both returned answers
r.ans.humgtp4x3 <- response %>% filter(!is.na(human) & !is.na(GPT4x3)) #214 instances where both returned answers
r.ans.humelicit <- response %>% filter(!is.na(human) & !is.na(Elicit)) #269 instances where both returned answers


## for context ##
c.hum_vs_GPT4x1<- context %>% filter(is.na(human) & !is.na(GPT4)) #56 instances where GPT4x1had response and human responded with NA
c.GPT4_vs_hum <- context %>% filter(is.na(GPT4) & !is.na(human)) #78 instances where human had response and GPT4x1responded with NA

c.hum_vs_GPT4x3 <- context %>% filter(is.na(human) & !is.na(GPT4x3)) #69 instances where GPT4x3 had response and human responded with NA
c.GPT4x3_vs_hum <- context %>% filter(is.na(GPT4x3) & !is.na(human)) #56 instances where human had response and GPT4x3 responded with NA

c.hum_vs_Elicit <- context %>% filter(is.na(human) & !is.na(Elicit)) #97 instances where Elicit had response and human responded with NA
c.Elicit_vs_hum <- context %>% filter(is.na(Elicit) & !is.na(human)) #1 instance where human had response and Elicit responded with NA

c.na.humGPT4x1<- context %>% filter(is.na(human) & is.na(GPT4)) #41 instances where both returned NA
c.na.humgtp4x3 <- context %>% filter(is.na(human) & is.na(GPT4x3)) #28 instances where both returned NA
c.na.humelicit <- context %>% filter(is.na(human) & is.na(Elicit)) #0 instances where both returned NA

c.ans.humgtp4 <- context %>% filter(!is.na(human) & !is.na(GPT4)) #188 instances where both returned answers
c.ans.humgtp4x3 <- context %>% filter(!is.na(human) & !is.na(GPT4x3)) #210 instances where both returned answers
c.ans.humelicit <- context %>% filter(!is.na(human) & !is.na(Elicit)) #265 instances where both returned answers


## creating confusion matrix from the data
pal = c("#a6611a",
        "#dfc27d",
        "#80cdc1",
        "#018571")

(newdfx <- newdf %>% filter(Category == 'response') %>% pivot_longer(c(GPT4, GPT4x3, Elicit), names_to = "AI", values_to = "value") %>% 
    mutate(human_data = ifelse(is.na(human),"No Data","Data"),
           ai_data = ifelse(is.na(value),"No Data","Data")) %>% 
    mutate(human_data = factor(human_data,levels = c('Data',"No Data")),
           ai_data = factor(ai_data, levels = c("Data","No Data") %>% rev)) %>% 
  #   
  # mutate(error_type = case_when(is.na(human) & is.na(value) ~ "4", 
  #                               is.na(human) & !is.na(value) ~ "extras",
  #                               !is.na(human) & is.na(value) ~ "misses",
  #                               !is.na(human) & !is.na(value) ~"finds",
  #                               TRUE ~ "0")) %>% 
  #   
  # mutate(human = factor(ifelse(str_detect(error_type,"misses|finds"),"Data","No Data"),levels = c("Data","No Data")),
  #        ai = factor(ifelse(str_detect(error_type,"extras|finds"),"Data","No Data"),levels = c("Data","No Data") %>% rev)) %>% 
  #  
    group_by(AI,human_data,ai_data,Category) %>% 
    summarise(value =  signif(n()/363, digits = 2)) %>% as_tibble() %>% 
    complete(AI,human_data,ai_data,Category, fill = list(value = 0)) %>% 
    ungroup() %>% 
    mutate(display_value = ifelse(value >= 0.01 | value == 0, value,"<0.01")) %>% 
  ggplot() +
  geom_tile(aes(x = human_data, y = ai_data, fill = interaction(human_data,ai_data))) +
  geom_text(aes(x = human_data, y = ai_data, label = display_value)) +
  xlab("Human") +
  ylab("AI") +
   scale_fill_manual(values = pal[c(1,4,4,2)]) + #c("tan2","forestgreen","darksalmon","forestgreen")) +
  # scale_x_discrete(position = "top") +
  facet_wrap(~ AI) +
  theme_classic() +
  theme(legend.position = "none"))

ggsave("Figures/F2_confusion_matrix.png", width = 7, height = 3, device = "png")

ggsave("Figures/F2_confusion_matrix.svg", width = 7, height = 3, device = "svg")

## list of false postives
FPdatax <- newdf %>% pivot_longer(c(GPT4, GPT4x3, Elicit), names_to = "AI", values_to = "value") %>% 
          mutate(human_data = ifelse(is.na(human),"No Data","Data"),
          ai_data = ifelse(is.na(value),"No Data","Data")) %>% 
          mutate(human_data = factor(human_data,levels = c('Data',"No Data")),
          ai_data = factor(ai_data, levels = c("Data","No Data") %>% rev))

FPdata <- FPdatax %>% filter(human_data == "No Data" & ai_data == "Data", Category == "context")

write.csv(FPdata, file = "falsepos_data.csv", row.names = FALSE)

```

#### overall plot showing all data
```{r overall}
overall <- ggplot(stat_summary %>% filter(group_agent == 'Human Assessor'), aes(x = AI, y = mean, #fill = Criteria, 
                                                                                group = Criteria)) +
    geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = 0), fill = "#FFC0CB", alpha = 0.1) +
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = 0, ymax = Inf), fill = "#98FB98", alpha = 0.1) +
  geom_col(stat = "identity", position = "dodge"#, color = "black"
           ) +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs( x = "AI Implementation", y = "Assessed Quality") +

  theme_classic() +
    geom_hline(yintercept = 0, linetype = 'dashed',alpha = 0.5 ) +

  coord_cartesian(ylim = c(-1, 1)) +
  facet_wrap(~Criteria) +
  #scale_y_continuous(    labels = c("Poor","Fair","Good"),breaks = c(1,2,3),) +
  scale_fill_manual(values = crit_pal) +
  theme(legend.position = 'bottom') 
plot(overall)

ggsave("Figures/F3_overall.svg", width = 7, height = 4, device = "svg")
ggsave("Figures/F3_overall.png", width = 7, height = 4, device = "png")
```

```{r overall-AI}
overall <- ggplot(stat_summary, aes(x = AI, y = mean,
                                    group = Criteria), fill = 'lightgrey') +
    geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = 0), fill = "#FFC0CB", alpha = 0.1) +
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = 0, ymax = Inf), fill = "#98FB98", alpha = 0.1) +
  geom_col(stat = "identity", position = "dodge"#, color = "black"
           ) +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs( x = "AI Implementation", y = "Assessed Quality") +

  theme_classic() +
    geom_hline(yintercept = 0, linetype = 'dashed',alpha = 0.5 ) +

  coord_cartesian(ylim = c(-1, 1)) +
  facet_wrap(group_agent~Criteria) +
  #scale_y_continuous(    labels = c("Poor","Fair","Good"),breaks = c(1,2,3),) +
  scale_fill_manual(values = crit_pal) +
  theme(legend.position = 'bottom') 
plot(overall)

ggsave("Figures/S1_overall-withAI.png", width = 7, height = 5, device = "png")
```

#### plot showing only human assessor data by AI
```{r human only}
# stat_summary_hum <- stat_summary %>% filter(group_agent == "Human Assessor") %>% filter(Criteria == "AI Response to Human Response")
# stat_summary_hum$significance <- ifelse(stat_summary_hum$AI == "GPT4x1" & stat_summary_hum$Criteria %in% c("AI Response to Human Response", "Context to Question"), "*", "")
# (human <- ggplot(stat_summary_hum, aes(x = AI, y = mean, fill = Criteria, group = Criteria)) +
#     geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = 0), fill = "#FFC0CB", alpha = 0.1) +
#   geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = 0, ymax = Inf), fill = "#98FB98", alpha = 0.1) +
#   geom_hline(yintercept = 0, linetype = 'dashed', alpha = 0.5) +
#   geom_col(stat = "identity", position = "dodge", color = "black") +
#   geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
#                 position = position_dodge(width = 0.9), width = 0.25) +
#   geom_text(aes(label = significance, y = 1.25), position = position_dodge(width = 0.9), vjust = 0, size = 10) +
#   # geom_text(aes(label = ifelse(AI == "Elicit" & Criteria == "AI Response to Human Response", "a", "")),
#   #          x = 1.3, y = 0.6, vjust = 0.5, size = 5, color = "black") +
#   # geom_text(aes(label = ifelse(AI == "GPT4x1" & Criteria == "Response to Context", "b", "")),
#   #           x = 2.3, y = 0.3, vjust = 0.5, size = 5, color = "black") +
#   # geom_text(aes(label = ifelse(AI == "GPT4x3" & Criteria == "Response to Context", "b", "")),
#   #         x = 3.3, y = 0.42, vjust = 0.5, size = 5, color = "black") +
#   labs(x = "AI Implementation", y = "Assessed Quality") +
#   theme_classic() +
#   coord_cartesian(ylim = c(-0.25, 0.5)) +
#   scale_fill_manual(values = crit_pal) +
#   theme(legend.position = 'bottom'))
# 
# ggsave("Figures/human.png", width = 7, height = 5, device = "png")
```

#### plot showing breakdown of assessment values by questions with overall difficulty
```{r questions}
q_plot <- question_df %>% filter(!str_detect(Reviewer,'GPT')) %>% ungroup() %>% 
  mutate(
    normalized_difficulty_mean = 1 + 2*((difficulty_mean - min(difficulty_mean,na.rm = TRUE)) / (max(difficulty_mean,na.rm = TRUE) - min(difficulty_mean,na.rm = TRUE))),
    normalized_difficulty_st_dev = st_dev_difficulty / (max(st_dev_difficulty) - min(st_dev_difficulty)),
    normalized_difficulty_standard_error = standard_error_difficulty / (max(standard_error_difficulty) - min(standard_error_difficulty))
  ) %>% 
  arrange(difficulty_mean) %>% ungroup() %>% 
  ggplot() +
  aes(x = AI, y = mean, fill = Criteria, group = Criteria) +
  geom_hline(yintercept = 2, linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  # geom_segment(aes(x = 4, y = 1.1, xend = 4, yend = 2.9), col = 'orange') +
  # geom_point(aes(x = 4,y = normalized_difficulty_mean), size = 3, col = 'orange', show.legend = FALSE) +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  # labs(title = "Mean Values by Group and Criteria", x = "Group", y = "Mean Value") +
  theme_classic() +
  coord_cartesian(ylim = c(-1, 1), xlim = c(1,3.7)) +
  facet_wrap(~Question, labeller = label_wrap_gen()) +
  labs(x = '') +
  scale_fill_manual(values = crit_pal) +
  scale_y_continuous(
    name = "Assessed Quality",
    #labels = c("Poor","Fair","Good"),breaks = c(1,2,3),
    # sec.axis = sec_axis(~ (. - 1)/2, name = "Question Difficulty", labels = c("Easy","Hard"), breaks = c(0,1)),
  ) +
  # theme(axis.text.y.right = element_text(color = "orange"),
  #       strip.text = element_text(size = 8),
  #       legend.position = 'bottom',
  # ) +
  geom_text(aes(x = 4.2, y = 1.15, label = Reviewer, hjust = 1, vjust = 2), size = 3) +
    theme(legend.position = 'bottom') +
    guides(color = guide_legend(ncol = 1),   # Stack legend items vertically
         fill = guide_legend(ncol = 1))
plot(q_plot)

ggsave("Figures/S2_questions.png", width = 9, height = 9, device = "png")

```

#### plot showing overall difficulty
```{r difficulty plot, echo = FALSE}
diff_plot <- ggplot(question_df %>% filter(Reviewer != 'GPT4-Turbo'), aes(x = difficulty_mean, y = mean, color = Criteria)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE#, aes(fill = Criteria),alpha = 0.1#se = FALSE
              ) +
  labs(x = "Mean Question Difficulty", y = "Mean Assessed Quality", color = "Criterion"#, shape = "AI Model"
       ) +
  # ggtitle("Relationship between Mean and Difficulty Mean by AI Model") +
  facet_wrap(~AI) +
  theme_classic() +
  theme(legend.position = 'bottom') +
    guides(color = guide_legend(ncol = 1),   # Stack legend items vertically
         fill = guide_legend(ncol = 1))
print(diff_plot)

ggsave("Figures/F4_question_difficulty.png", width = 6, height = 5, device = "png")
ggsave("Figures/F4_question_difficulty.svg", width = 6, height = 5, device = "svg")

# diff_plot2 <- ggplot(statdfx, aes(x = Difficulty, y = value, color = AI#, shape = AI)) +
#   geom_point() +
#   geom_smooth(method = "lm") +
#   labs(x = "Difficulty", y = "Value", color = "AI Model"#, shape = "AI Model"
#        ) +
#   ggtitle("Relationship between Mean and Difficulty Mean by AI Model")
#print(diff_plot2) this plot is irrelevant because it's only 9 points


```

#### plot showing difficulty broken up by each AI model
```{r difficulty by AI}
diffbyai <- ggplot(question_df %>% filter(!str_detect(Reviewer,'GPT')), aes(x = difficulty_mean, y = mean, col = Criteria)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +  # Use method = "lm" for linear model
  #stat_poly_line() +
  #stat_poly_eq() +
  theme_classic() +
  facet_wrap(~AI) +
  labs(y = 'Mean Assessed Quality', x = 'Mean Question Difficulty') +
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = crit_pal)
plot(diffbyai)

ggsave("Figures/diff_by_ai.png", width = 7, height = 5, device = "png")
```

#### plot showing how reviewers ranked the same dataset, compared to AI (gpt4-turbo)
```{r reviewer consistency}
rev <- ggplot(reviewer_df, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
  geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
  geom_col(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(x = "Reviewer", y = "Mean Assessed Quality") +
  theme_classic() +
  coord_cartesian(ylim = c(-1, 1)) +
  theme(legend.position = 'bottom') +
  #facet_wrap(~Question) +
  scale_fill_manual(values = crit_pal)  # Set your desired fill colors
plot(rev)
```
