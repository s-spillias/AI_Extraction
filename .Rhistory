x = "AI",
y = "t-value") +
theme_minimal()
plot(tplot)
model_extractor <- lm(mean ~ Extractor, data = filter(diffdata, Criteria == "AI Response to Human Response"))
diffdata <- paper_df %>% filter(group_agent != "GPT4-Turbo Assessor") %>% mutate(short_cit = substr(Citation, 1,35))
diffdata <- diffdata %>% left_join(select(extractor, short_cit, Extractor), by = c("short_cit"))
diffdata <- subset(diffdata, select = -short_cit)
model_extractor <- lm(mean ~ Extractor, data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(model_extractor)
# examining each criteria independently
aieffect <- lmer(value ~ AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
summary(aieffect)
anova(aieffect)
aieffect_emmeans <- emmeans(aieffect, ~ AI)
aieffect_cont <- pairs(aieffect_emmeans, adjust = "tukey")
print(aieffect_cont)
anova(aieffect)
qeffect <- lmer(value ~ Question + (1|Reviewer) + (1|Citation) + (1|AI), data = filter(statdfx, Criteria == "AI Response to Human Response"))
anova(qeffect)
diff <- lmer(value ~ Difficulty*AI + (1|Citation) + (1|Extractor) + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
anova(diff)
assessor <- lmer(value ~ group_agent + (1|Extractor) + (1|Reviewer), data = full_df)
anova(assessor)
# open files - create dataframes #
file_path = 'output_assessment_CHATGPT_explanation.xlsx'
validate_path = list.files('Raw_Team_Assessment', pattern = 'validate', full.names = TRUE)
figure_folder = '../Manuscript/Figures'
# firstcontact-gpt4-32k
# January 29, ran Elicit and GPT4-32k
sheet_names <- excel_sheets(file_path)
all_data_list <- lapply(sheet_names, function(sheet) {
read_excel(file_path, sheet = sheet) %>%
mutate(AI = sheet)})
do_stats = function(x){
x %>%
summarise(mean = mean(value),
standard_error = sd(value, na.rm = TRUE) / sqrt(n()),
st_dev = sd(value),
difficulty_mean = mean(Difficulty, na.rm = TRUE),
st_dev_difficulty = sd(Difficulty, na.rm = TRUE),
standard_error_difficulty = sd(Difficulty, na.rm = TRUE) / sqrt(n()),
)}
difficulty <- read_excel("Extraction_Human_Difficulty.xlsx") %>% as_tibble() %>%
pivot_longer(-Question) %>%
setNames(c('Citation','Question','Difficulty')) %>%
mutate(Difficulty = case_match(Difficulty, 'Easy' ~ 1, 'Medium' ~ 2, 'Hard' ~ 3))
file_path = 'Extraction_ai.xlsx'
sheets = excel_sheets(file_path)
ai_extractions <- sheets %>% lapply(function(x) read_excel(file_path, sheet = x) %>% as_tibble() %>%
mutate(across(everything(), function(x) x %>% str_replace("Response: ",""))) %>%
separate_wider_delim(-Citation, delim = "Context:", names = c('Response','Context'), names_sep = '***',too_few = "align_start") %>%
mutate(across(-Citation, nchar))) %>% setNames(sheets)
human_extraction = "Human_Extraction_Cleaned.xlsx" %>%
read_excel(sheet = 'Human') %>%
select(-5, -(seq(8, ncol(.), by = 3))) %>%
select(-1) %>%
slice(-(1:2)) %>%
mutate(across(-Question, nchar)) %>%
select(-last_col())
verbosity_ratios = sheets %>% lapply(function(x) bind_cols(human_extraction$Question, (ai_extractions[[x]] %>% select(-1))/(human_extraction %>% select(-1))))
reviewers = c(
'R1', "Which country was the study conducted in?",
'R1',"Provide some background as to the drivers and/or motivators of community-based fisheries management.",
'R2',"What management mechanisms are used?",
'R3',"Which groups of people are involved in the management as part of the CBFM case-studies? Choices: Community-members, Researchers, Practitioners, Government, NGOs, Other",
'R4',"What benefits of Community-Based Fisheries Management are reported in this case study?",
'R4',"What are the indicators of success of CBFM?",
'R1',"How was the data on benefits collected?",
'R3',"What are the reported barriers to success of Community-Based Fisheries Management?",
'R2',"Guidelines for future implementation of CBFM?",
'R3',"How does the community monitor the system they are managing?",
'R4',"How does the community make decisions?")
extractor <- "Human_Extraction_Cleaned.xlsx" %>%
read_excel(sheet = 'Human') %>%
dplyr::select(-5, -(seq(8, ncol(.), by = 3))) %>%
#dplyr::select(-1) %>%
dplyr::slice(-(1:2)) %>%
mutate(across(-Question, nchar)) %>%
dplyr::select(-last_col()) %>%
select(-3:-24) %>%
mutate(short_cit = substr(Question, 1,35)) #no idea why citation column is actually the question column in this instance
colnames(extractor)[1] <- "Extractor"
colnames(extractor)[2] <- "Citation"
colnames(extractor)[3] <- "short_cit"
reviewers_id <- matrix(reviewers, nrow = length(reviewers) / 2, byrow = TRUE) %>% as_tibble() %>%
setNames(c("Reviewer","Question"))
df_description = all_data_list %>% bind_rows() %>%
pivot_longer(-c('Citation','AI')) %>%
separate(value, sep = ':::', into = c("value", "description")) %>%
separate(value, sep = "\\|", into = paste0("rep",1:5)) %>%
separate(description, sep = "\\|", into = paste0("Description",1:5)) %>%
pivot_longer(starts_with('rep'), names_to = 'Agent')
ai_df <- df_description %>%
dplyr::select(-starts_with('Description')) %>%
separate(value, sep = ';', into = paste0('Crit', 1:4)) %>%
pivot_longer(starts_with('Crit'), names_to = 'Criteria')
team_path = "output_assessment_TEAM.xlsx" #original file with unedited flag values = output_assessment_TEAM_OG.xlsx
sheet_names <- excel_sheets(team_path)
team_data_list <- lapply(sheet_names, function(sheet) {
read_excel(team_path, sheet = sheet) %>%
mutate(AI = sheet)
})
df_validate = validate_path %>%
lapply(function(x) sheet_names %>% lapply(function(y)    read_excel(x, sheet = y) %>%
mutate(AI = y, Reviewer = x %>% str_remove("Raw_Team_Assessment/validate_assessment_") %>% str_remove('.xlsx')) %>%
bind_rows())
) %>% bind_rows() %>%
pivot_longer(-c('Citation','AI','Reviewer'), names_to = 'Question',values_to = 'Value') %>%
filter(!(Value %>% str_detect("Response|Context"))) %>%
pivot_wider(names_from = 'Reviewer', values_from = 'Value') %>%
filter(!is.na(SCOTT)) %>%
pivot_longer(-c('Citation','AI','Question'), names_to = 'Reviewer',values_to = 'Value') %>%
separate(Value, sep = ';', into = paste0('Crit',1:4)) %>%
pivot_longer(starts_with('crit'),values_to = 'value', names_to = 'Criteria') %>%
mutate(value = as.numeric(value)) %>%
mutate(Reviewer = recode(Reviewer, 'MATT' = 'R1',
'FABIO' = 'R2',
'SCOTT' = 'R3',
'ROWAN' = 'R4'))
stat_summary <- team_data_list %>% bind_rows() %>%
pivot_longer(-c("Citation","AI")) %>%
separate(value, sep = ';', into = paste0('Crit',1:4)) %>%
pivot_longer(starts_with("Crit"), names_to = 'Criteria') %>%
mutate(Agent = 'Human') %>%
bind_rows(ai_df) %>%
mutate(Flag = ifelse(Criteria == 'Crit4' & value > 1, "FLAG",NA),
group_agent = ifelse(str_detect(Agent, "Human"), "Human Assessor", "GPT4-Turbo Assessor"),
value = as.integer(value)) %>%
{flag_df <<- .} %>%
filter(!is.na(value)) %>%
filter(Criteria != 'Crit4') %>%
relocate(value, .after = 'group_agent') %>%
mutate(Criteria = case_match(Criteria, 'Crit1' ~ 'Context to Question',
'Crit2' ~ 'Response to Context',
'Crit3' ~ 'AI Response to Human Response')) %>%
rename('Question' = 'name') %>%
left_join(reviewers_id, by = 'Question') %>%
mutate(Reviewer = ifelse(str_detect(group_agent,'GPT'),'GPT4-Turbo',Reviewer)) %>%
mutate(short_cit = substr(Citation, 1,35)) %>%
left_join(difficulty %>%  mutate(short_cit = substr(Citation, 1,35)) %>% dplyr::select(-Citation), by = c("Question",'short_cit')) %>%
mutate(AI = case_match(AI, 'ELICIT' ~ 'Elicit',
'OPENAI' ~ 'GPT4x3',
'OPENAI_single' ~ 'GPT4x1')) %>%
mutate(Question = factor(str_split_fixed(Question, '\\?',2)[,1],levels = str_split_fixed(reviewers_id$Question, '\\?',2)[,1])) %>%
{full_df <<- .} %>%
group_by(group_agent, AI,Criteria) %>%
do_stats()
full_df <- full_df %>%
left_join(extractor, by = "short_cit") %>%
mutate(Extractor = ifelse(group_agent == "Human Assessor", Extractor, "AI")) %>%
select(-Citation.y) %>%
rename(Citation = Citation.x)
question_df <- full_df %>%
group_by(Reviewer, AI, Criteria, Question) %>%
do_stats()
paper_df <- full_df %>%
group_by(group_agent, AI, Criteria, Citation) %>%
do_stats()
reviewer_df <- full_df %>%
group_by(Reviewer, Criteria) %>%
do_stats()
flag_df <- flag_df %>% filter(!is.na(Flag))
humandf <- full_df %>% filter(Agent == "Human") %>% select(-short_cit)
aidf <- full_df %>% filter(Agent != "Human")
aidf2 <- aidf %>% group_by(Citation, AI, Question, Criteria, Flag, group_agent, Reviewer, Difficulty, Extractor) %>%
summarise(value = median(value)) %>% mutate(Agent = "AI") %>%
select(Citation, AI, Question, Criteria, Agent, Flag, group_agent, value, Reviewer, Difficulty, Extractor)
statdf <- rbind(humandf, aidf2)
statdfx <- statdf %>% filter(Agent == "Human")
diffdata <- paper_df %>% filter(group_agent != "GPT4-Turbo Assessor") %>% mutate(short_cit = substr(Citation, 1,35))
diffdata <- diffdata %>% left_join(select(extractor, short_cit, Extractor), by = c("short_cit"))
diffdata <- subset(diffdata, select = -short_cit)
ttest <- statdfx %>% filter (Criteria == "AI Response to Human Response") %>% group_by(AI) %>%  summarise(t_value = t.test(value, mu = 2, alternative = "two.sided")$statistic, p_value = t.test(value, mu = 2, alternative = "two.sided")$p.value)
print(ttest)
tplot <- ggplot(ttest, aes(x = AI, y = t_value)) +
geom_bar(stat = "identity", position = "dodge") +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
geom_text(aes(label = ifelse(p_value < 0.05, "*", "")), position = position_dodge(width = 0.9), vjust = -0.5) +
labs(title = "Comparison to Mean of 2",
x = "AI",
y = "t-value") +
theme_minimal()
plot(tplot)
model_extractor <- lm(mean ~ Extractor, data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(model_extractor)
# examining each criteria independently
aieffect <- lmer(value ~ AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
summary(aieffect)
anova(aieffect)
aieffect_emmeans <- emmeans(aieffect, ~ AI)
aieffect_cont <- pairs(aieffect_emmeans, adjust = "tukey")
print(aieffect_cont)
anova(aieffect)
qeffect <- lmer(value ~ Question + (1|Reviewer) + (1|Citation) + (1|AI), data = filter(statdfx, Criteria == "AI Response to Human Response"))
anova(qeffect)
diff <- lmer(value ~ Difficulty*AI + (1|Citation) + (1|Extractor) + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
anova(diff)
assessor <- lmer(value ~ group_agent + (1|Extractor) + (1|Reviewer), data = full_df)
anova(assessor)
View(full_df)
# open files - create dataframes #
file_path = 'output_assessment_CHATGPT_explanation.xlsx'
validate_path = list.files('Raw_Team_Assessment', pattern = 'validate', full.names = TRUE)
figure_folder = '../Manuscript/Figures'
# firstcontact-gpt4-32k
# January 29, ran Elicit and GPT4-32k
sheet_names <- excel_sheets(file_path)
all_data_list <- lapply(sheet_names, function(sheet) {
read_excel(file_path, sheet = sheet) %>%
mutate(AI = sheet)})
do_stats = function(x){
x %>%
summarise(mean = mean(value),
standard_error = sd(value, na.rm = TRUE) / sqrt(n()),
st_dev = sd(value),
difficulty_mean = mean(Difficulty, na.rm = TRUE),
st_dev_difficulty = sd(Difficulty, na.rm = TRUE),
standard_error_difficulty = sd(Difficulty, na.rm = TRUE) / sqrt(n()),
)}
difficulty <- read_excel("Extraction_Human_Difficulty.xlsx") %>% as_tibble() %>%
pivot_longer(-Question) %>%
setNames(c('Citation','Question','Difficulty')) %>%
mutate(Difficulty = case_match(Difficulty, 'Easy' ~ 1, 'Medium' ~ 2, 'Hard' ~ 3))
file_path = 'Extraction_ai.xlsx'
sheets = excel_sheets(file_path)
ai_extractions <- sheets %>% lapply(function(x) read_excel(file_path, sheet = x) %>% as_tibble() %>%
mutate(across(everything(), function(x) x %>% str_replace("Response: ",""))) %>%
separate_wider_delim(-Citation, delim = "Context:", names = c('Response','Context'), names_sep = '***',too_few = "align_start") %>%
mutate(across(-Citation, nchar))) %>% setNames(sheets)
human_extraction = "Human_Extraction_Cleaned.xlsx" %>%
read_excel(sheet = 'Human') %>%
select(-5, -(seq(8, ncol(.), by = 3))) %>%
select(-1) %>%
slice(-(1:2)) %>%
mutate(across(-Question, nchar)) %>%
select(-last_col())
verbosity_ratios = sheets %>% lapply(function(x) bind_cols(human_extraction$Question, (ai_extractions[[x]] %>% select(-1))/(human_extraction %>% select(-1))))
reviewers = c(
'R1', "Which country was the study conducted in?",
'R1',"Provide some background as to the drivers and/or motivators of community-based fisheries management.",
'R2',"What management mechanisms are used?",
'R3',"Which groups of people are involved in the management as part of the CBFM case-studies? Choices: Community-members, Researchers, Practitioners, Government, NGOs, Other",
'R4',"What benefits of Community-Based Fisheries Management are reported in this case study?",
'R4',"What are the indicators of success of CBFM?",
'R1',"How was the data on benefits collected?",
'R3',"What are the reported barriers to success of Community-Based Fisheries Management?",
'R2',"Guidelines for future implementation of CBFM?",
'R3',"How does the community monitor the system they are managing?",
'R4',"How does the community make decisions?")
extractor <- "Human_Extraction_Cleaned.xlsx" %>%
read_excel(sheet = 'Human') %>%
dplyr::select(-5, -(seq(8, ncol(.), by = 3))) %>%
#dplyr::select(-1) %>%
dplyr::slice(-(1:2)) %>%
mutate(across(-Question, nchar)) %>%
dplyr::select(-last_col()) %>%
select(-3:-24) %>%
mutate(short_cit = substr(Question, 1,35)) #no idea why citation column is actually the question column in this instance
colnames(extractor)[1] <- "Extractor"
colnames(extractor)[2] <- "Citation"
colnames(extractor)[3] <- "short_cit"
reviewers_id <- matrix(reviewers, nrow = length(reviewers) / 2, byrow = TRUE) %>% as_tibble() %>%
setNames(c("Reviewer","Question"))
df_description = all_data_list %>% bind_rows() %>%
pivot_longer(-c('Citation','AI')) %>%
separate(value, sep = ':::', into = c("value", "description")) %>%
separate(value, sep = "\\|", into = paste0("rep",1:5)) %>%
separate(description, sep = "\\|", into = paste0("Description",1:5)) %>%
pivot_longer(starts_with('rep'), names_to = 'Agent')
ai_df <- df_description %>%
dplyr::select(-starts_with('Description')) %>%
separate(value, sep = ';', into = paste0('Crit', 1:4)) %>%
pivot_longer(starts_with('Crit'), names_to = 'Criteria')
team_path = "output_assessment_TEAM.xlsx" #original file with unedited flag values = output_assessment_TEAM_OG.xlsx
sheet_names <- excel_sheets(team_path)
team_data_list <- lapply(sheet_names, function(sheet) {
read_excel(team_path, sheet = sheet) %>%
mutate(AI = sheet)
})
df_validate = validate_path %>%
lapply(function(x) sheet_names %>% lapply(function(y)    read_excel(x, sheet = y) %>%
mutate(AI = y, Reviewer = x %>% str_remove("Raw_Team_Assessment/validate_assessment_") %>% str_remove('.xlsx')) %>%
bind_rows())
) %>% bind_rows() %>%
pivot_longer(-c('Citation','AI','Reviewer'), names_to = 'Question',values_to = 'Value') %>%
filter(!(Value %>% str_detect("Response|Context"))) %>%
pivot_wider(names_from = 'Reviewer', values_from = 'Value') %>%
filter(!is.na(SCOTT)) %>%
pivot_longer(-c('Citation','AI','Question'), names_to = 'Reviewer',values_to = 'Value') %>%
separate(Value, sep = ';', into = paste0('Crit',1:4)) %>%
pivot_longer(starts_with('crit'),values_to = 'value', names_to = 'Criteria') %>%
mutate(value = as.numeric(value)) %>%
mutate(Reviewer = recode(Reviewer, 'MATT' = 'R1',
'FABIO' = 'R2',
'SCOTT' = 'R3',
'ROWAN' = 'R4'))
stat_summary <- team_data_list %>% bind_rows() %>%
pivot_longer(-c("Citation","AI")) %>%
separate(value, sep = ';', into = paste0('Crit',1:4)) %>%
pivot_longer(starts_with("Crit"), names_to = 'Criteria') %>%
mutate(Agent = 'Human') %>%
bind_rows(ai_df) %>%
mutate(Flag = ifelse(Criteria == 'Crit4' & value > 1, "FLAG",NA),
group_agent = ifelse(str_detect(Agent, "Human"), "Human Assessor", "GPT4-Turbo Assessor"),
value = as.integer(value)) %>%
{flag_df <<- .} %>%
filter(!is.na(value)) %>%
filter(Criteria != 'Crit4') %>%
relocate(value, .after = 'group_agent') %>%
mutate(Criteria = case_match(Criteria, 'Crit1' ~ 'Context to Question',
'Crit2' ~ 'Response to Context',
'Crit3' ~ 'AI Response to Human Response')) %>%
rename('Question' = 'name') %>%
left_join(reviewers_id, by = 'Question') %>%
mutate(Reviewer = ifelse(str_detect(group_agent,'GPT'),'GPT4-Turbo',Reviewer)) %>%
mutate(short_cit = substr(Citation, 1,35)) %>%
left_join(difficulty %>%  mutate(short_cit = substr(Citation, 1,35)) %>% dplyr::select(-Citation), by = c("Question",'short_cit')) %>%
mutate(AI = case_match(AI, 'ELICIT' ~ 'Elicit',
'OPENAI' ~ 'GPT4x3',
'OPENAI_single' ~ 'GPT4x1')) %>%
mutate(Question = factor(str_split_fixed(Question, '\\?',2)[,1],levels = str_split_fixed(reviewers_id$Question, '\\?',2)[,1])) %>%
{full_df <<- .} %>%
group_by(group_agent, AI,Criteria) %>%
do_stats()
full_df <- full_df %>%
left_join(extractor, by = "short_cit") %>%
mutate(Extractor = ifelse(group_agent == "Human Assessor", Extractor, "AI")) %>%
select(-Citation.y) %>%
rename(Citation = Citation.x)
question_df <- full_df %>%
group_by(Reviewer, AI, Criteria, Question) %>%
do_stats()
paper_df <- full_df %>%
group_by(group_agent, AI, Criteria, Citation) %>%
do_stats()
reviewer_df <- full_df %>%
group_by(Reviewer, Criteria) %>%
do_stats()
flag_df <- flag_df %>% filter(!is.na(Flag))
full_df$value <- full_df$value - 2
humandf <- full_df %>% filter(Agent == "Human") %>% select(-short_cit)
aidf <- full_df %>% filter(Agent != "Human")
aidf2 <- aidf %>% group_by(Citation, AI, Question, Criteria, Flag, group_agent, Reviewer, Difficulty, Extractor) %>%
summarise(value = median(value)) %>% mutate(Agent = "AI") %>%
select(Citation, AI, Question, Criteria, Agent, Flag, group_agent, value, Reviewer, Difficulty, Extractor)
statdf <- rbind(humandf, aidf2)
statdfx <- statdf %>% filter(Agent == "Human")
diffdata <- paper_df %>% filter(group_agent != "GPT4-Turbo Assessor") %>% mutate(short_cit = substr(Citation, 1,35))
diffdata <- diffdata %>% left_join(select(extractor, short_cit, Extractor), by = c("short_cit"))
diffdata <- subset(diffdata, select = -short_cit)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
library(tidyverse)
library(readxl)
library(beyonce)
library(ggpubr)
library(lme4)
library(lmerTest)
options(max.print = 999999)
# open files - create dataframes #
file_path = 'output_assessment_CHATGPT_explanation.xlsx'
validate_path = list.files('Raw_Team_Assessment', pattern = 'validate', full.names = TRUE)
figure_folder = '../Manuscript/Figures'
# firstcontact-gpt4-32k
# January 29, ran Elicit and GPT4-32k
sheet_names <- excel_sheets(file_path)
all_data_list <- lapply(sheet_names, function(sheet) {
read_excel(file_path, sheet = sheet) %>%
mutate(AI = sheet)})
do_stats = function(x){
x %>%
summarise(mean = mean(value),
standard_error = sd(value, na.rm = TRUE) / sqrt(n()),
st_dev = sd(value),
difficulty_mean = mean(Difficulty, na.rm = TRUE),
st_dev_difficulty = sd(Difficulty, na.rm = TRUE),
standard_error_difficulty = sd(Difficulty, na.rm = TRUE) / sqrt(n()),
)}
difficulty <- read_excel("Extraction_Human_Difficulty.xlsx") %>% as_tibble() %>%
pivot_longer(-Question) %>%
setNames(c('Citation','Question','Difficulty')) %>%
mutate(Difficulty = case_match(Difficulty, 'Easy' ~ 1, 'Medium' ~ 2, 'Hard' ~ 3))
file_path = 'Extraction_ai.xlsx'
sheets = excel_sheets(file_path)
ai_extractions <- sheets %>% lapply(function(x) read_excel(file_path, sheet = x) %>% as_tibble() %>%
mutate(across(everything(), function(x) x %>% str_replace("Response: ",""))) %>%
separate_wider_delim(-Citation, delim = "Context:", names = c('Response','Context'), names_sep = '***',too_few = "align_start") %>%
mutate(across(-Citation, nchar))) %>% setNames(sheets)
human_extraction = "Human_Extraction_Cleaned.xlsx" %>%
read_excel(sheet = 'Human') %>%
select(-5, -(seq(8, ncol(.), by = 3))) %>%
select(-1) %>%
slice(-(1:2)) %>%
mutate(across(-Question, nchar)) %>%
select(-last_col())
verbosity_ratios = sheets %>% lapply(function(x) bind_cols(human_extraction$Question, (ai_extractions[[x]] %>% select(-1))/(human_extraction %>% select(-1))))
reviewers = c(
'R1', "Which country was the study conducted in?",
'R1',"Provide some background as to the drivers and/or motivators of community-based fisheries management.",
'R2',"What management mechanisms are used?",
'R3',"Which groups of people are involved in the management as part of the CBFM case-studies? Choices: Community-members, Researchers, Practitioners, Government, NGOs, Other",
'R4',"What benefits of Community-Based Fisheries Management are reported in this case study?",
'R4',"What are the indicators of success of CBFM?",
'R1',"How was the data on benefits collected?",
'R3',"What are the reported barriers to success of Community-Based Fisheries Management?",
'R2',"Guidelines for future implementation of CBFM?",
'R3',"How does the community monitor the system they are managing?",
'R4',"How does the community make decisions?")
extractor <- "Human_Extraction_Cleaned.xlsx" %>%
read_excel(sheet = 'Human') %>%
dplyr::select(-5, -(seq(8, ncol(.), by = 3))) %>%
#dplyr::select(-1) %>%
dplyr::slice(-(1:2)) %>%
mutate(across(-Question, nchar)) %>%
dplyr::select(-last_col()) %>%
select(-3:-24) %>%
mutate(short_cit = substr(Question, 1,35)) #no idea why citation column is actually the question column in this instance
colnames(extractor)[1] <- "Extractor"
colnames(extractor)[2] <- "Citation"
colnames(extractor)[3] <- "short_cit"
reviewers_id <- matrix(reviewers, nrow = length(reviewers) / 2, byrow = TRUE) %>% as_tibble() %>%
setNames(c("Reviewer","Question"))
df_description = all_data_list %>% bind_rows() %>%
pivot_longer(-c('Citation','AI')) %>%
separate(value, sep = ':::', into = c("value", "description")) %>%
separate(value, sep = "\\|", into = paste0("rep",1:5)) %>%
separate(description, sep = "\\|", into = paste0("Description",1:5)) %>%
pivot_longer(starts_with('rep'), names_to = 'Agent')
ai_df <- df_description %>%
dplyr::select(-starts_with('Description')) %>%
separate(value, sep = ';', into = paste0('Crit', 1:4)) %>%
pivot_longer(starts_with('Crit'), names_to = 'Criteria')
team_path = "output_assessment_TEAM.xlsx" #original file with unedited flag values = output_assessment_TEAM_OG.xlsx
sheet_names <- excel_sheets(team_path)
team_data_list <- lapply(sheet_names, function(sheet) {
read_excel(team_path, sheet = sheet) %>%
mutate(AI = sheet)
})
df_validate = validate_path %>%
lapply(function(x) sheet_names %>% lapply(function(y)    read_excel(x, sheet = y) %>%
mutate(AI = y, Reviewer = x %>% str_remove("Raw_Team_Assessment/validate_assessment_") %>% str_remove('.xlsx')) %>%
bind_rows())
) %>% bind_rows() %>%
pivot_longer(-c('Citation','AI','Reviewer'), names_to = 'Question',values_to = 'Value') %>%
filter(!(Value %>% str_detect("Response|Context"))) %>%
pivot_wider(names_from = 'Reviewer', values_from = 'Value') %>%
filter(!is.na(SCOTT)) %>%
pivot_longer(-c('Citation','AI','Question'), names_to = 'Reviewer',values_to = 'Value') %>%
separate(Value, sep = ';', into = paste0('Crit',1:4)) %>%
pivot_longer(starts_with('crit'),values_to = 'value', names_to = 'Criteria') %>%
mutate(value = as.numeric(value)) %>%
mutate(Reviewer = recode(Reviewer, 'MATT' = 'R1',
'FABIO' = 'R2',
'SCOTT' = 'R3',
'ROWAN' = 'R4'))
stat_summary <- team_data_list %>% bind_rows() %>%
pivot_longer(-c("Citation","AI")) %>%
separate(value, sep = ';', into = paste0('Crit',1:4)) %>%
pivot_longer(starts_with("Crit"), names_to = 'Criteria') %>%
mutate(Agent = 'Human') %>%
bind_rows(ai_df) %>%
mutate(Flag = ifelse(Criteria == 'Crit4' & value > 1, "FLAG",NA),
group_agent = ifelse(str_detect(Agent, "Human"), "Human Assessor", "GPT4-Turbo Assessor"),
value = as.integer(value)) %>%
{flag_df <<- .} %>%
filter(!is.na(value)) %>%
filter(Criteria != 'Crit4') %>%
relocate(value, .after = 'group_agent') %>%
mutate(Criteria = case_match(Criteria, 'Crit1' ~ 'Context to Question',
'Crit2' ~ 'Response to Context',
'Crit3' ~ 'AI Response to Human Response')) %>%
rename('Question' = 'name') %>%
left_join(reviewers_id, by = 'Question') %>%
mutate(Reviewer = ifelse(str_detect(group_agent,'GPT'),'GPT4-Turbo',Reviewer)) %>%
mutate(short_cit = substr(Citation, 1,35)) %>%
left_join(difficulty %>%  mutate(short_cit = substr(Citation, 1,35)) %>% dplyr::select(-Citation), by = c("Question",'short_cit')) %>%
mutate(AI = case_match(AI, 'ELICIT' ~ 'Elicit',
'OPENAI' ~ 'GPT4x3',
'OPENAI_single' ~ 'GPT4x1')) %>%
mutate(Question = factor(str_split_fixed(Question, '\\?',2)[,1],levels = str_split_fixed(reviewers_id$Question, '\\?',2)[,1])) %>%
{full_df <<- .} %>%
group_by(group_agent, AI,Criteria) %>%
do_stats()
full_df <- full_df %>%
left_join(extractor, by = "short_cit") %>%
mutate(Extractor = ifelse(group_agent == "Human Assessor", Extractor, "AI")) %>%
select(-Citation.y) %>%
rename(Citation = Citation.x)
question_df <- full_df %>%
group_by(Reviewer, AI, Criteria, Question) %>%
do_stats()
paper_df <- full_df %>%
group_by(group_agent, AI, Criteria, Citation) %>%
do_stats()
reviewer_df <- full_df %>%
group_by(Reviewer, Criteria) %>%
do_stats()
flag_df <- flag_df %>% filter(!is.na(Flag))
full_df$value <- full_df$value - 2
humandf <- full_df %>% filter(Agent == "Human") %>% select(-short_cit)
aidf <- full_df %>% filter(Agent != "Human")
aidf2 <- aidf %>% group_by(Citation, AI, Question, Criteria, Flag, group_agent, Reviewer, Difficulty, Extractor) %>%
summarise(value = median(value)) %>% mutate(Agent = "AI") %>%
select(Citation, AI, Question, Criteria, Agent, Flag, group_agent, value, Reviewer, Difficulty, Extractor)
statdf <- rbind(humandf, aidf2)
statdfx <- statdf %>% filter(Agent == "Human")
diffdata <- paper_df %>% filter(group_agent != "GPT4-Turbo Assessor") %>% mutate(short_cit = substr(Citation, 1,35))
diffdata <- diffdata %>% left_join(select(extractor, short_cit, Extractor), by = c("short_cit"))
diffdata <- subset(diffdata, select = -short_cit)
