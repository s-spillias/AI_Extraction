library(dplyr)
library(tidyverse)
library(readxl)
library(beyonce)
library(ggpubr)
library(lme4)
library(lmerTest)
options(max.print = 999999)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
# open files - create dataframes #
file_path = 'output_assessment_CHATGPT_explanation.xlsx'
validate_path = list.files('Raw_Team_Assessment', pattern = 'validate', full.names = TRUE)
figure_folder = '../Manuscript/Figures'
# firstcontact-gpt4-32k
# January 29, ran Elicit and GPT4-32k
sheet_names <- excel_sheets(file_path)
all_data_list <- lapply(sheet_names, function(sheet) {
read_excel(file_path, sheet = sheet) %>%
mutate(AI = sheet)})
do_stats = function(x){
x %>%
summarise(mean = mean(value),
standard_error = sd(value, na.rm = TRUE) / sqrt(n()),
st_dev = sd(value),
difficulty_mean = mean(Difficulty, na.rm = TRUE),
st_dev_difficulty = sd(Difficulty, na.rm = TRUE),
standard_error_difficulty = sd(Difficulty, na.rm = TRUE) / sqrt(n()),
)}
difficulty <- read_excel("Extraction_Human_Difficulty.xlsx") %>% as_tibble() %>%
pivot_longer(-Question) %>%
setNames(c('Citation','Question','Difficulty')) %>%
mutate(Difficulty = case_match(Difficulty, 'Easy' ~ 1, 'Medium' ~ 2, 'Hard' ~ 3))
file_path = 'Extraction_ai.xlsx'
sheets = excel_sheets(file_path)
ai_extractions <- sheets %>% lapply(function(x) read_excel(file_path, sheet = x) %>% as_tibble() %>%
mutate(across(everything(), function(x) x %>% str_replace("Response: ",""))) %>%
separate_wider_delim(-Citation, delim = "Context:", names = c('Response','Context'), names_sep = '***',too_few = "align_start") %>%
mutate(across(-Citation, nchar))) %>% setNames(sheets)
human_extraction = "Human_Extraction_Cleaned.xlsx" %>%
read_excel(sheet = 'Human') %>%
select(-5, -(seq(8, ncol(.), by = 3))) %>%
select(-1) %>%
slice(-(1:2)) %>%
mutate(across(-Question, nchar)) %>%
select(-last_col())
verbosity_ratios = sheets %>% lapply(function(x) bind_cols(human_extraction$Question, (ai_extractions[[x]] %>% select(-1))/(human_extraction %>% select(-1))))
reviewers = c(
'R1', "Which country was the study conducted in?",
'R1',"Provide some background as to the drivers and/or motivators of community-based fisheries management.",
'R2',"What management mechanisms are used?",
'R3',"Which groups of people are involved in the management as part of the CBFM case-studies? Choices: Community-members, Researchers, Practitioners, Government, NGOs, Other",
'R4',"What benefits of Community-Based Fisheries Management are reported in this case study?",
'R4',"What are the indicators of success of CBFM?",
'R1',"How was the data on benefits collected?",
'R3',"What are the reported barriers to success of Community-Based Fisheries Management?",
'R2',"Guidelines for future implementation of CBFM?",
'R3',"How does the community monitor the system they are managing?",
'R4',"How does the community make decisions?")
extractor <- "Human_Extraction_Cleaned.xlsx" %>%
read_excel(sheet = 'Human') %>%
dplyr::select(-5, -(seq(8, ncol(.), by = 3))) %>%
#dplyr::select(-1) %>%
dplyr::slice(-(1:2)) %>%
mutate(across(-Question, nchar)) %>%
dplyr::select(-last_col()) %>%
select(-3:-24) %>%
mutate(short_cit = substr(Question, 1,35)) #no idea why citation column is actually the question column in this instance
colnames(extractor)[1] <- "Extractor"
colnames(extractor)[2] <- "Citation"
colnames(extractor)[3] <- "short_cit"
reviewers_id <- matrix(reviewers, nrow = length(reviewers) / 2, byrow = TRUE) %>% as_tibble() %>%
setNames(c("Reviewer","Question"))
df_description = all_data_list %>% bind_rows() %>%
pivot_longer(-c('Citation','AI')) %>%
separate(value, sep = ':::', into = c("value", "description")) %>%
separate(value, sep = "\\|", into = paste0("rep",1:5)) %>%
separate(description, sep = "\\|", into = paste0("Description",1:5)) %>%
pivot_longer(starts_with('rep'), names_to = 'Agent')
ai_df <- df_description %>%
dplyr::select(-starts_with('Description')) %>%
separate(value, sep = ';', into = paste0('Crit', 1:4)) %>%
pivot_longer(starts_with('Crit'), names_to = 'Criteria')
team_path = "output_assessment_TEAM.xlsx" #original file with unedited flag values = output_assessment_TEAM_OG.xlsx
sheet_names <- excel_sheets(team_path)
team_data_list <- lapply(sheet_names, function(sheet) {
read_excel(team_path, sheet = sheet) %>%
mutate(AI = sheet)
})
df_validate = validate_path %>%
lapply(function(x) sheet_names %>% lapply(function(y)    read_excel(x, sheet = y) %>%
mutate(AI = y, Reviewer = x %>% str_remove("Raw_Team_Assessment/validate_assessment_") %>% str_remove('.xlsx')) %>%
bind_rows())
) %>% bind_rows() %>%
pivot_longer(-c('Citation','AI','Reviewer'), names_to = 'Question',values_to = 'Value') %>%
filter(!(Value %>% str_detect("Response|Context"))) %>%
pivot_wider(names_from = 'Reviewer', values_from = 'Value') %>%
filter(!is.na(SCOTT)) %>%
pivot_longer(-c('Citation','AI','Question'), names_to = 'Reviewer',values_to = 'Value') %>%
separate(Value, sep = ';', into = paste0('Crit',1:4)) %>%
pivot_longer(starts_with('crit'),values_to = 'value', names_to = 'Criteria') %>%
mutate(value = as.numeric(value)) %>%
mutate(Reviewer = recode(Reviewer, 'MATT' = 'R1',
'FABIO' = 'R2',
'SCOTT' = 'R3',
'ROWAN' = 'R4'))
stat_summary <- team_data_list %>% bind_rows() %>%
pivot_longer(-c("Citation","AI")) %>%
separate(value, sep = ';', into = paste0('Crit',1:4)) %>%
pivot_longer(starts_with("Crit"), names_to = 'Criteria') %>%
mutate(Agent = 'Human') %>%
bind_rows(ai_df) %>%
mutate(Flag = ifelse(Criteria == 'Crit4' & value > 1, "FLAG",NA),
group_agent = ifelse(str_detect(Agent, "Human"), "Human Assessor", "GPT4-Turbo Assessor"),
value = as.integer(value)) %>%
{flag_df <<- .} %>%
filter(!is.na(value)) %>%
filter(Criteria != 'Crit4') %>%
relocate(value, .after = 'group_agent') %>%
mutate(Criteria = case_match(Criteria, 'Crit1' ~ 'Context to Question',
'Crit2' ~ 'Response to Context',
'Crit3' ~ 'AI Response to Human Response')) %>%
rename('Question' = 'name') %>%
left_join(reviewers_id, by = 'Question') %>%
mutate(Reviewer = ifelse(str_detect(group_agent,'GPT'),'GPT4-Turbo',Reviewer)) %>%
mutate(short_cit = substr(Citation, 1,35)) %>%
left_join(difficulty %>%  mutate(short_cit = substr(Citation, 1,35)) %>% dplyr::select(-Citation), by = c("Question",'short_cit')) %>%
mutate(AI = case_match(AI, 'ELICIT' ~ 'Elicit',
'OPENAI' ~ 'GPT4x3',
'OPENAI_single' ~ 'GPT4x1')) %>%
mutate(Question = factor(str_split_fixed(Question, '\\?',2)[,1],levels = str_split_fixed(reviewers_id$Question, '\\?',2)[,1])) %>%
{full_df <<- .} %>%
group_by(group_agent, AI,Criteria) %>%
do_stats()
full_df <- full_df %>%
left_join(extractor, by = "short_cit") %>%
mutate(Extractor = ifelse(group_agent == "Human Assessor", Extractor, "AI")) %>%
select(-Citation.y) %>%
rename(Citation = Citation.x)
question_df <- full_df %>%
group_by(Reviewer, AI, Criteria, Question) %>%
do_stats()
paper_df <- full_df %>%
group_by(group_agent, AI, Criteria, Citation) %>%
do_stats()
reviewer_df <- full_df %>%
group_by(Reviewer, Criteria) %>%
do_stats()
flag_df <- flag_df %>% filter(!is.na(Flag))
### Make Plots ####
crit_pal = beyonce_palette(127)[-1]
## Overall
ggplot(stat_summary, aes(x = AI, y = mean, fill = Criteria, group = Criteria)) +
geom_hline(yintercept = 2, linetype = 'dashed',alpha = 0.5 ) +
geom_col(stat = "identity", position = "dodge", color = "black") +
geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
position = position_dodge(width = 0.9), width = 0.25) +
labs( x = "AI Implementation", y = "Assessed Quality") +
theme_classic() +
coord_cartesian(ylim = c(1, 3)) +
facet_wrap(~group_agent) +
#scale_y_continuous(    labels = c("Poor","Fair","Good"),breaks = c(1,2,3),) +
scale_fill_manual(values = crit_pal) +
theme(legend.position = 'bottom')
#ggsave(file.path(figure_folder,'overall.png'))
## Questions
question_df %>% filter(!str_detect(Reviewer,'GPT')) %>% ungroup() %>%
mutate(
normalized_difficulty_mean = 1 + 2*((difficulty_mean - min(difficulty_mean,na.rm = TRUE)) / (max(difficulty_mean,na.rm = TRUE) - min(difficulty_mean,na.rm = TRUE))),
normalized_difficulty_st_dev = st_dev_difficulty / (max(st_dev_difficulty) - min(st_dev_difficulty)),
normalized_difficulty_standard_error = standard_error_difficulty / (max(standard_error_difficulty) - min(standard_error_difficulty))
) %>%
arrange(difficulty_mean) %>% ungroup() %>%
ggplot() +
aes(x = AI, y = mean, fill = Criteria, group = Criteria) +
geom_hline(yintercept = 2, linetype = 'dashed',alpha = 0.5 ) +
geom_col(stat = "identity", position = "dodge", color = "black") +
geom_segment(aes(x = 4, y = 1.1, xend = 4, yend = 2.9), col = 'orange') +
geom_point(aes(x = 4,y = normalized_difficulty_mean), size = 3, col = 'orange', show.legend = FALSE) +
geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
position = position_dodge(width = 0.9), width = 0.25) +
# labs(title = "Mean Values by Group and Criteria", x = "Group", y = "Mean Value") +
theme_classic() +
coord_cartesian(ylim = c(1, 3), xlim = c(1,3.7)) +
facet_wrap(~Question, labeller = label_wrap_gen()) +
labs(x = '') +
scale_fill_manual(values = crit_pal) +
scale_y_continuous(
name = "Assessed Quality",
#labels = c("Poor","Fair","Good"),breaks = c(1,2,3),
sec.axis = sec_axis(~ (. - 1)/2, name = "Question Difficulty", labels = c("Easy","Hard"), breaks = c(0,1)),
) +
theme(axis.text.y.right = element_text(color = "orange"),
strip.text = element_text(size = 8),
legend.position = 'bottom',
) +
geom_text(aes(x = 4.2, y = 1.15, label = Reviewer, hjust = 1, vjust = 1), size = 3)
#ggsave(file.path(figure_folder,'questions_supp.png'), width = 12, height = 7)
#Check for Reviewer Consistency and validation
ggplot(reviewer_df, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
geom_col(stat = "identity", position = "dodge", color = "black") +
geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
position = position_dodge(width = 0.9), width = 0.25) +
labs(x = "Reviewer", y = "Mean Assessed Quality") +
theme_classic() +
coord_cartesian(ylim = c(1, 3)) +
theme(legend.position = 'bottom') +
#facet_wrap(~Question) +
scale_fill_manual(values = crit_pal)  # Set your desired fill colors
#ggsave(file.path(figure_folder,'reviewer_supp.png'), width = 12, height = 7)
df_validate_stats = df_validate %>%
filter(Criteria !='Crit4') %>%
group_by(Criteria, Reviewer) %>%
summarise(mean = mean(value),
standard_error = sd(value, na.rm = TRUE) / sqrt(n()),
st_dev = sd(value))
ggplot(df_validate_stats, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
geom_col(stat = "identity", position = "dodge", color = "black") +
geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
position = position_dodge(width = 0.9), width = 0.25) +
labs(x = "Reviewer", y = "Mean Assessed Quality") +
theme_classic() +
coord_cartesian(ylim = c(1, 3)) +
theme(legend.position = 'bottom') +
#facet_wrap(~Question) +
scale_fill_manual(values = crit_pal)
df_validate %>% filter(Criteria != 'Crit4') %>% ggplot() +
geom_bar(stat = 'identity', aes(x = Criteria, y = value, fill = Reviewer), position = 'dodge') +
facet_wrap(AI~Question)
full_df %>% filter(!str_detect(Reviewer,'GPT')) %>%
group_by(Reviewer,Criteria) %>%
ggplot() +
geom_density(fill = 'skyblue',aes(x = value), adjust = 2.5) +
facet_grid(Reviewer~ Criteria) +
theme_classic()
##Impact of Difficulty
ggplot(question_df %>% filter(!str_detect(Reviewer,'GPT')), aes(x = difficulty_mean, y = mean, col = Criteria)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +  # Use method = "lm" for linear model
#stat_poly_line() +
#stat_poly_eq() +
theme_classic() +
facet_wrap(~AI) +
labs(y = 'Mean Assessed Quality', x = 'Mean Question Difficulty') +
theme(legend.position = 'bottom') +
scale_colour_manual(values = crit_pal)
#ggsave(file.path(figure_folder,'question_difficulty_supp.png'), width = 12, height = 7)
ggplot(paper_df %>% filter(str_detect(group_agent,'Human')),
aes(x = difficulty_mean, y = mean, col = AI)) +
geom_point() +
labs(y = 'Mean Assessed Quality', x = 'Mean Paper Difficulty') +
geom_smooth(method = "lm", se = FALSE) +  # Use method = "lm" for linear model
theme_classic() +
theme(legend.position = 'bottom') +
scale_colour_manual(values = crit_pal) # Set your desired fill colors
#ggsave(file.path(figure_folder,'paper_difficulty_supp.png'), width = 12, height = 7)
#### analysis ####
# combining median value for the 5 replicates of the AI assessor
humandf <- full_df %>% filter(Agent == "Human") %>% select(-short_cit)
aidf <- full_df %>% filter(Agent != "Human")
aidf2 <- aidf %>% group_by(Citation, AI, Question, Criteria, Flag, group_agent, Reviewer, Difficulty, Extractor) %>%
summarise(value = median(value)) %>% mutate(Agent = "AI") %>%
select(Citation, AI, Question, Criteria, Agent, Flag, group_agent, value, Reviewer, Difficulty, Extractor)
statdf <- rbind(humandf, aidf2)
# anova of this - may be INVALID due to distribution of data
aiaov <- aov(data = statdf, value ~ AI)
summary(aiaov)
tt <- TukeyHSD(aiaov)
tt
plot(aiaov)
# validating reviewers #
statdf2 <- statdf %>% filter(Reviewer != "GPT4-Turbo")
val <- aov( data = statdf2, value ~ Reviewer*Criteria)
summary(val)
valtt <- TukeyHSD(val)
valtt
df_validate_stats <- df_validate_stats %>% mutate(Criteria = case_match(Criteria, 'Crit1' ~ 'Context to Question',
'Crit2' ~ 'Response to Context',
'Crit3' ~ 'AI Response to Human Response'))
valplot <- {ggplot(df_validate_stats, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
geom_col(stat = "identity", position = "dodge", color = "black") +
geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
position = position_dodge(width = 0.9), width = 0.25) +
labs(x = "Reviewer", y = "Mean Assessed Quality") +
theme_classic() +
coord_cartesian(ylim = c(1, 3)) +
theme(legend.position = 'bottom') +
#facet_wrap(~Question) +
scale_fill_manual(values = crit_pal)} #will need to run process_evaluations_new before to get objects
df_val <- df_validate %>% filter(Criteria != "Crit4") %>% mutate(Criteria = case_match(Criteria, 'Crit1' ~ 'Context to Question',
'Crit2' ~ 'Response to Context',
'Crit3' ~ 'AI Response to Human Response'))
crit1_mean <- df_val %>% filter(Criteria == "Context to Question") # crit 1 - in red
crit1_mean <- mean(crit1_mean$value)
print(crit1_mean) #mean = 2.458
crit2_mean <- df_val %>% filter(Criteria == "Response to Context") #crit 2 - in green
crit2_mean <- mean(crit2_mean$value)
print(crit2_mean) #mean = 2.533
crit3_mean <- df_val %>% filter(Criteria == "AI Response to Human Response") #crit 3 - in blue
crit3_mean <- mean(crit3_mean$value)
print(crit3_mean) #mean = 2.242
# attempting to create 5th "reviewer" which is all combined data - I think is valid (talk with Dave) but not best approach
df_val2 <- df_val %>% filter(Criteria != "NA") %>% mutate(Reviewer = "R5")
valbind <- rbind(df_val, df_val2)
valbind_stats = valbind %>%
group_by(Criteria, Reviewer) %>%
summarise(mean = mean(value),
standard_error = sd(value, na.rm = TRUE) / sqrt(n()),
st_dev = sd(value))
valplot2 <- {ggplot(valbind_stats, aes(x = Reviewer, y = mean, fill = Criteria, group = Criteria)) +
geom_hline(yintercept = 2,linetype = 'dashed',alpha = 0.5 ) +
geom_col(stat = "identity", position = "dodge", color = "black") +
geom_errorbar(aes(ymin = mean - standard_error, ymax = mean + standard_error),
position = position_dodge(width = 0.9), width = 0.25) +
labs(x = "Reviewer", y = "Mean Assessed Quality") +
theme_classic() +
coord_cartesian(ylim = c(1, 3)) +
theme(legend.position = 'bottom') +
#facet_wrap(~Question) +
scale_fill_manual(values = crit_pal)} # compares individual reviewers (1-4) to the mean of all the values (R5)
valaov <- aov(data = valbind, value ~ Reviewer*Criteria)
summary(valaov)
val_hsd <- TukeyHSD(valaov)
val_hsd
#only reviewer 1 is sig. diff. from "reviewer 5" (the average of all data) WHEN ALL CRITERIA COMBINED
#no interaction between reviewer*criteria (p = 0.222) - only sig. diff is R1 vs R3 response to context
## LMM analysis ##
library(lme4)
lmm_model <- lmer(value ~ AI + (1|Reviewer), data = statdf)
summary(lmm_model)
lmm_mod_anova <- anova(lmm_model)
#TukeyHSD(lmm_mod_anova)
## GLMM analysis ##
library(lme4)
library(emmeans)
statdfx <- statdf %>% filter(Agent == "Human")
glmm_mod <- glmer(value ~ AI + (1|Reviewer), data = statdfx, family = poisson) #but is poisson legit? only one that works
summary(glmm_mod)
anova(glmm_mod)
emm <- emmeans(glmm_mod, ~ AI)
contrasts <- pairs(emm, adjust = "tukey")
print(contrasts)
glmm_mod2 <- glmer(value ~ AI*Criteria + (1|Reviewer), data = statdf, family = poisson) #but is poisson legit? only one that works
summary(glmm_mod2)
anova(glmm_mod2)
emm2 <- emmeans(glmm_mod2, ~ AI*Criteria)
contrasts2 <- pairs(emm2, adjust = "tukey")
print(contrasts2)
glmm_mod <- glmer(value ~ group_agent*AI + (1|Reviewer), data = statdf, family = poisson) #but is poisson legit? only one that works
summary(glmm_mod)
anova(glmm_mod)
emm <- emmeans(glmm_mod, ~ group_agent*AI)
contrasts <- pairs(emm, adjust = "tukey")
print(contrasts)
mod_glmm <- glmer(value ~ AI*group_agent + (1|Reviewer), data = statdf, family = poisson)
summary(mod_glmm)
anova(mod_glmm)
emm_glmm <- emmeans(mod_glmm, ~ AI|group_agent)
pairs(emm_glmm)
emm_int_glmm <- emmeans(mod_glmm, pairwise ~ AI:group_agent)
pairs(emm_int_glmm)
# comparing full model with interaction to reduced model without - if p < 0.05, then interaction between AI and group_agent is sig.
mod_red <- glmer(value ~ AI + group_agent + (1|Reviewer), data = statdf, family = poisson)
anova_res <- anova(mod_red, mod_glmm)
print(anova_res)
#### secondary analysis ####
statdfx <- statdf %>% filter(Agent == "Human")
xx <- glmer(value ~ AI + (1|Reviewer), family = poisson, data = filter(statdfx, Criteria == "AI Response to Human Response"))
#only comparing the value to the intercept with reviewer as the random effect
attempt1 <- lmer(value ~ 1 + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
summary(attempt1)
randoms <- ranef(attempt1, condVar = TRUE) #determining intercepts
#comparing the value by each AI (reviewer still random)
attempt2 <- lmer(value ~ AI + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
summary(attempt2)
att2 <- emmeans(attempt2, ~ AI)
contrasts <- pairs(att2, adjust = "tukey")
print(contrasts)
#anova comparing the two models
anova(attempt1, attempt2) #AIC lower for attempt 2, better, meaning AI has an effect
#now including question as a random effect
attempt3 <- lmer(value ~ AI + (1|Question) + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
anova(attempt2, attempt3)
summary(attempt3)
#### PROPER MODEL TESTS ####
statdfx <- statdf %>% filter(Agent == "Human") #only investigating human assessors (no group agent)
#comparing distribution of values to the mean of 2
statdfx %>% group_by(AI, Criteria) %>% summarise(t_value = t.test(value, mu = 2, alternative = "less")$statistic)
statdfx %>%  group_by(AI, Criteria) %>%  summarise(t_value = t.test(value, mu = 2, alternative = "less")$statistic, p_value = t.test(value, mu = 2, alternative = "less")$p.value)
output1 <- statdfx %>%  group_by(AI, Criteria) %>%  summarise(t_value = t.test(value, mu = 2, alternative = "less")$statistic, p_value = t.test(value, mu = 2, alternative = "less")$p.value)
ungroup <- ungroup(output1)
#effect of difficulty on values for AI response to human response
diffdata <- paper_df %>% filter(group_agent != "GPT4-Turbo Assessor") %>% mutate(short_cit = substr(Citation, 1,35))
diffdata <- diffdata %>% left_join(select(extractor, short_cit, Extractor), by = c("short_cit"))
diffdata <- subset(diffdata, select = -short_cit)
write.csv(diffdata, "diffdata.csv")
cor.test(diffdata$mean, diffdata$difficulty_mean) #correlation test to see if mean and difficulty mean are related (p = 0.48 - not sig.)
diffeffect <- lmer(mean ~ difficulty_mean + AI + (1|Citation), data = diffdata)
summ <- summary(diffeffect)
anova(diffeffect)
diff_null <- lmer(mean ~ difficulty_mean + (1|Citation), data = diffdata)
summary(diff_null)
anova(diff_null)
anova(diff_null, diffeffect)
diff_plot <- ggplot(diffdata, aes(x = difficulty_mean, y = mean, color = AI, shape = AI)) +
geom_point() +
geom_smooth(method = "lm") +
labs(x = "Difficulty Mean", y = "Mean", color = "AI Model", shape = "AI Model") +
ggtitle("Relationship between Mean and Difficulty Mean by AI Model")
#effect of AI on values for AI response to human response
aieffect <- lmer(value ~ AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "Context to Question"))
summary(aieffect)
aieffect_emmeans <- emmeans(aieffect, ~ AI)
aieffect_cont <- pairs(aieffect_emmeans, adjust = "tukey")
print_aieffect <- print(aieffect_cont)
#effect of question on values for AI response to human response
qeffect <- lmer(value ~ Question + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
summary(qeffect)
diffdata <- paper_df %>% filter(group_agent != "GPT4-Turbo Assessor") %>% mutate(short_cit = substr(Citation, 1,35))
diffdata <- diffdata %>% left_join(select(extractor, short_cit, Extractor), by = c("short_cit"))
diffdata <- subset(diffdata, select = -short_cit)
write.csv(diffdata, "diffdata.csv")
cor.test(diffdata$mean, diffdata$difficulty_mean)
extrac_eff <- lmer(mean ~ Extractor + (1|Citation), data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(extrac_eff)
cit_eff <- lmer(mean ~ Citation + (1|Extractor), data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(cit_eff)
# xxx <- lm(mean ~ Extractor, data = filter(diffdata, Criteria == "AI Response to Human Response"))
# anova(xxx)
View(extractor)
View(diffdata)
write.csv(diffdata, file = "diffdata.csv")
model_extractor <- lm(mean ~ Extractor, data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(model_extractor)
model_citation <- lm(mean ~ Citation, data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(model_citation)
anova(extrac_eff)
anova(model_extractor)
#Citation
cit_eff <- lmer(mean ~ Citation + (1|Extractor), data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(cit_eff)
cit_eff <- lmer(mean ~ Citation + (1|Extractor), data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(cit_eff)
model_citation <- lm(mean ~ Citation, data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(model_citation)
model_citation <- lm(mean ~ Citation, data = filter(diffdata, Criteria == "AI Response to Human Response"))
anova(model_citation)
ttest <- statdfx %>% filter (Criteria == "AI Response to Human Response") %>% group_by(AI) %>%  summarise(t_value = t.test(value, mu = 2, alternative = "two.sided")$statistic, p_value = t.test(value, mu = 2, alternative = "two.sided")$p.value)
print(ttest)
aieffect <- lmer(value ~ AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
summary(aieffect)
anova(aieffect)
aieffect_emmeans <- emmeans(aieffect, ~ AI)
aieffect_cont <- pairs(aieffect_emmeans, adjust = "tukey")
print(aieffect_cont)
qeffect <- lmer(value ~ Question*AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
#summary(qeffect)
anova(qeffect)
qeff_emmeans <- emmeans(qeffect, ~Question)
qeff_cont <- pairs(qeff_emmeans, adjust = "tukey")
print(qeff_cont)
qeff_contdataframe <- as.data.frame(qeff_cont)
write.csv(qeff_contdataframe, "qeff_AIrespHumresp.csv")
qeffect <- lmer(value ~ Question + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
#summary(qeffect)
anova(qeffect)
qeff_emmeans <- emmeans(qeffect, ~Question)
qeff_cont <- pairs(qeff_emmeans, adjust = "tukey")
print(qeff_cont)
qeff_contdataframe <- as.data.frame(qeff_cont)
write.csv(qeff_contdataframe, "qeff_AIrespHumresp.csv")
qeffect <- lmer(value ~ Question*AI + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
#summary(qeffect)
anova(qeffect)
qeff_emmeans <- emmeans(qeffect, ~Question*AI)
qeff_cont <- pairs(qeff_emmeans, adjust = "tukey")
print(qeff_cont)
qeff_contdataframe <- as.data.frame(qeff_cont)
write.csv(qeff_contdataframe, "qeff_AIrespHumresp.csv")
View(qeff_contdataframe)
qeffect <- lmer(value ~ Question + (1|Reviewer) + (1|Citation), data = filter(statdfx, Criteria == "AI Response to Human Response"))
#summary(qeffect)
anova(qeffect)
qeff_emmeans <- emmeans(qeffect, ~Question)
qeff_cont <- pairs(qeff_emmeans, adjust = "tukey")
print(qeff_cont)
qeff_contdataframe <- as.data.frame(qeff_cont)
write.csv(qeff_contdataframe, "qeff_AIrespHumresp.csv")
statdfx2 <- statdfx %>% filter(Criteria == "AI Response to Human Response")
diff <- lmer(value ~ Difficulty*AI + (1|Citation) + (1|Extractor) + (1|Reviewer), data = statdfx2)
anova(diff)
summary(diff)
# diff_emmeans <- emmeans(diff, ~AI)
# diff_cont <- pairs(diff_emmeans, adjust = "tukey")
# print(diff_cont)
# ALL CRITERIA - JUST FOR COMPARISON
diff <- lmer(value ~ Difficulty*Criteria + (1|Citation) + (1|Extractor) + (1|Reviewer), data = statdfx)
anova(diff)
statdfx2 <- statdfx %>% filter(Criteria == "AI Response to Human Response")
diff <- lmer(value ~ Difficulty*AI + (1|Citation) + (1|Extractor) + (1|Reviewer), data = statdfx2)
anova(diff)
summary(diff)
diff <- lmer(value ~ Difficulty*Criteria + (1|Citation) + (1|Extractor) + (1|Reviewer), data = statdfx)
anova(diff)
diff <- lmer(value ~ Difficulty*AI + (1|Citation) + (1|Extractor) + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
anova(diff)
summary(diff)
diff <- lmer(value ~ Difficulty*AI + (1|Citation) + (1|Extractor) + (1|Reviewer), data = filter(statdfx, Criteria == "AI Response to Human Response"))
anova(diff)
summary(diff)
